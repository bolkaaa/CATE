<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-03-03T15:57:13+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">CATE</title><subtitle>Project blog for the CATE program, updated with my progress and findings.</subtitle><author><name>Liam Wyllie</name></author><entry><title type="html">Discrete Fourier Transforms and Starting on Qt GUI</title><link href="http://localhost:4000/2019/02/24/discrete-fourier-transforms-and-starting-qt-gui.html" rel="alternate" type="text/html" title="Discrete Fourier Transforms and Starting on Qt GUI" /><published>2019-02-24T00:00:00+00:00</published><updated>2019-02-24T00:00:00+00:00</updated><id>http://localhost:4000/2019/02/24/discrete-fourier-transforms-and-starting-qt-gui</id><content type="html" xml:base="http://localhost:4000/2019/02/24/discrete-fourier-transforms-and-starting-qt-gui.html">&lt;h3 id=&quot;this-week&quot;&gt;This Week&lt;/h3&gt;
&lt;p&gt;This week, I’ve been working on the following parts of my program:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Discrete Fourier Transforms&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Starting on the Qt GUI&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;More Work on Python Utilities&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Creating a More Generalised Audio Buffer Class&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;implementing-discrete-fourier-transforms&quot;&gt;Implementing Discrete Fourier Transforms&lt;/h3&gt;
&lt;p&gt;For spectral audio analysis and visualisation, having some way of converting my
audio sample data to the frequency domain will be essential.&lt;/p&gt;

&lt;p&gt;My requirements for a FFT library were that it should be a well-established,
standard library used by high-profile open source programs, and that it should
only offer functionality for doing discrete fourier transforms and not other
unnecessary DSP functions that I was already implementing myself. The only
library that really met this criteria was &lt;a href=&quot;http://www.fftw.org/&quot;&gt;fftw&lt;/a&gt;. This is
a very significant library within open source/free-as-in-freedom software,
having been used by a lot of audio and signal processing applications in
general. I felt that it was a good opportunity for me to learn how to use it.
The only downside was that it’s natively a C library rather than a C++ library,
but I could not find any natively C++ alternatives.&lt;/p&gt;

&lt;p&gt;The library can be quite unforgiving to use, with lots of potential for
segmentation faults and other issues with array access due to how memory works
in C. After some experimentation, I eventually got a good system working. The
fftw library works in the following way:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Allocate memory for input and output arrays&lt;/li&gt;
  &lt;li&gt;Create a “plan” that defines the operations carried out, e.g
&lt;em&gt;complex-to-complex&lt;/em&gt; or &lt;em&gt;real-to-complex&lt;/em&gt;, as well as specifying the arrays
created in the last step.&lt;/li&gt;
  &lt;li&gt;Call the fftw_execute function to get the output spectrum array, which can be
called as many times as needed (e.g, in a real-time loop).&lt;/li&gt;
  &lt;li&gt;Deallocate memory and call fftw_destroy_plan when done (e.g, when audio stream is finished).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I used the &lt;a href=&quot;http://www.fftw.org/fftw3_doc/One_002dDimensional-DFTs-of-Real-Data.html&quot;&gt;one-dimensional
real-to-complex&lt;/a&gt;
plan, which converts a one-dimensional input of floating point numbers to an
output array of complex numbers. The specified input array is &lt;em&gt;n&lt;/em&gt; real numbers,
and the output is &lt;em&gt;n/2+1&lt;/em&gt; complex numbers, which means the input array needs to
be filled with &lt;em&gt;N/2+1&lt;/em&gt; elements and then padded with zeros. Additionally, the
array of data should be windowed, to avoid discontinuities at the starts and
ends. I just included an application of the Hann function within the function
that fills the data buffer, which is defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w(n) = \frac{1} {2} \left(1 - cos\left(\frac {2\pi n} {N - 1}\right)\right)&lt;/script&gt;

&lt;p&gt;where &lt;em&gt;n&lt;/em&gt; is the sample index and &lt;em&gt;N&lt;/em&gt; is the total number of samples.&lt;/p&gt;

&lt;p&gt;Generally when using DFT data, the magnitude of the resultant complex output is taken. This can be calculated as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sqrt{r^2 + i^2}&lt;/script&gt;

&lt;p&gt;where &lt;em&gt;r&lt;/em&gt; and &lt;em&gt;i&lt;/em&gt; are the real and imaginary parts of a complex number
respectively. When using std::complex in C++, you can just use the std::abs
function from the standard library to get the magnitude, making for a more
elegant implementation.&lt;/p&gt;

&lt;p&gt;To use my spectrum array in a meaningful way, I will need to convert the
indices to their respective frequencies. The following formula can be used for
converting bin index to frequency:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{n  \frac{Fs} {2}} {N}&lt;/script&gt;

&lt;p&gt;where &lt;em&gt;n&lt;/em&gt; is the bin index, &lt;em&gt;Fs&lt;/em&gt; is the sampling rate and &lt;em&gt;N&lt;/em&gt; is the total number
of bins. Hence, the 512th bin represents the magnitude of the frequency 11025Hz,
given a bin size of 1024 and a sample rate of 44100.&lt;/p&gt;

&lt;h3 id=&quot;starting-on-the-qt-gui&quot;&gt;Starting on the Qt GUI&lt;/h3&gt;
&lt;p&gt;I decided to commit to using Qt in my project and start working on it ahead of
schedule. This is mostly because I’ve realised that the backend of the code will
need to be redesigned quite a lot around the general practises of Qt
applications, such as the idea of QObjects. My reasons for using Qt are mostly
that it’s a well-established GUI library/framework that has been used by a lot
of applications, including notable audio software like
&lt;a href=&quot;https://supercollider.github.io&quot;&gt;SuperCollider&lt;/a&gt; and
&lt;a href=&quot;https://www.sonicvisualiser.org/&quot;&gt;SonicVisualiser&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I had to alter my CMake configuration to get Qt working, using the &lt;a href=&quot;https://github.com/euler0/mini-cmake-qt&quot;&gt;following
github repo&lt;/a&gt; as a guideline.&lt;/p&gt;

&lt;p&gt;I then aimed to get a program working where my audio process is running in a
background with a Qt window interface in the foreground. I started with
&lt;a href=&quot;https://github.com/fabienpn/simple-qt-thread-example&quot;&gt;the following example from a github
repo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I also found &lt;a href=&quot;http://www.martin-kumm.de/wiki/doku.php?id=05Misc:A_Template_for_Audio_DSP_Applications&quot;&gt;this
example&lt;/a&gt;
of combining PortAudio, Qt and FFTW which was perfect for what I’m trying to do.&lt;/p&gt;

&lt;p&gt;Learning from those examples, I now have the Qt graphics framework implemented
in my program. The program now launches as a window with buttons for starting
and stopping the audio engine. The downside of this, is that I was forced to
revert my audio code to use the C API of PortAudio rather than the C++ version.
Integrating the C++ PortAudio code with Qt was proving troublesome, and it was
considerably easier to wrap the low-level C code instead. I did want to avoid
this, but after reading some discussions in the PortAudio mailing list, this
seems to be what is generally done by people working with PortAudio in C++
anyway.&lt;/p&gt;

&lt;p&gt;As for Qt, I will leave the actual bulk of the GUI design until much later, but
I want to get Qt integrated into my program as soon as possible. Hopefully
within the next month or so I will have a working prototype of the final system.
Then, I can improve upon it while working on the GUI. The final GUI will likely
be fairly simple, as I am new to Qt and it could be very time consuming to
create something complex in it. Additionally, the idea for my program is that it
should have a minimal interface and not require a lot of user input anyway!&lt;/p&gt;

&lt;h3 id=&quot;creating-a-more-generalised-audio-buffer-class&quot;&gt;Creating a More Generalised Audio Buffer Class&lt;/h3&gt;
&lt;p&gt;I have also been thinking about the overall design of my program and its data
flow. As I’ve mostly been working on audio input, ring buffer and FFT code
recently, the earlier code I had written to read and write data from sound files
has become somewhat incompatible with the new code. My idea now is to have
interoperability between my classes that work with sample data. RingBuffer, FFT,
and AudioBuffer objects filled with data from audio files should be able to
communicate and share data easily. Therefore, I have looked into making my
AudioBuffer class a base class for the others and using inheritance to achieve
this.&lt;/p&gt;

&lt;h3 id=&quot;more-python-utilities&quot;&gt;More Python Utilities&lt;/h3&gt;
&lt;p&gt;I also did a bit more work on the simple Python programs in the scripts
directory. I plan to compare the results of my C++ program’s outputs for things
like magnitude spectrum and analysis data. With libraries like
&lt;a href=&quot;https://librosa.github.io/librosa/&quot;&gt;librosa&lt;/a&gt;, I can check what the output
should look like for these processes very quickly and easily. My other
motivation for doing this, is related to the overall ethos I have with this in
project in thinking in the long-term as well as about the project itself. I
think that improving my proficiency at using libraries with librosa and Python
prototyping in general will be a useful skill for me in the future.&lt;/p&gt;

&lt;h3 id=&quot;moving-onwards&quot;&gt;Moving Onwards&lt;/h3&gt;
&lt;p&gt;My plan for the next week is to experiment with creating my own audio analysis
system with fftw and other low-level libraries. As a fallback option, I have
installed the Essentia library on my system and I will use it if this becomes
too much work. I am also thinking ahead to some of the next steps I will need to
take, such as the nearest-neighbours search of the parameter-space once I have
my analysis functionality sorted out. I have covered the K-Nearest neighbours
algorithm in other classes, where I learned that the K-d tree data structure
allows for a more efficient implementation of the algorithm, which will likely
be necessary so I can perform the comparisons efficiently in the audio loop.&lt;/p&gt;

&lt;p&gt;Additionally, I want to implement better error handling with my PortAudio code
and clean it up in general. I will refrain from doing a lot of GUI work right
now, although I might add some kind of text output in the main window for
debugging pourposes.&lt;/p&gt;</content><author><name>Liam Wyllie</name></author><summary type="html">This Week This week, I’ve been working on the following parts of my program:</summary></entry><entry><title type="html">Moving Onwards: Real-time Audio Analysis</title><link href="http://localhost:4000/2019/02/17/going-forwards-with-real-time-audio-analysis.html" rel="alternate" type="text/html" title="Moving Onwards: Real-time Audio Analysis" /><published>2019-02-17T00:00:00+00:00</published><updated>2019-02-17T00:00:00+00:00</updated><id>http://localhost:4000/2019/02/17/going-forwards-with-real-time-audio-analysis</id><content type="html" xml:base="http://localhost:4000/2019/02/17/going-forwards-with-real-time-audio-analysis.html">&lt;h3 id=&quot;thoughts-on-the-programs-direction&quot;&gt;Thoughts on the Program’s Direction&lt;/h3&gt;
&lt;p&gt;I have been thinking a lot about my aims with the program and my report, and the
particular niche of musical concatenative synthesis I would like to explore. One
aspect I would like to focus more strongly on, is the notion of working with
real-time audio input. Thus, CATE would primarily be an experimental tool for
generating variations of an audio source in real-time. Essentially, this will be
a type of audio effect, where the incoming audio is transformed according to its
analysed features.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;CataRT&lt;/em&gt; offers this functionality, but overall it’s a more general-purpose
realisation of corpus-based concatenative synthesis in a musical context. I
would aim to focus on this aspect of the technique and see how it can be
expanded upon and made more usable by electronic music practitioners such as
myself. One aspect that would be important to me, would be for the system to
easily “playable” by a solo performer/composer. Thus, the control of the system
should require little intervention after an initial configuration process. It
may be useful to do more research into current studies in improvisation-based
music software systems. The paper &lt;em&gt;Electro/Acoustic Improvisation and Deeply
Listening Machines&lt;/em&gt; touches upon some ideas related to this.&lt;/p&gt;

&lt;p&gt;Ideally, the relationship between the source sound and the new sound
concatenated from the corpus should be identifiable. As discussed with my
supervisor, one potentially interesting approach could be for there to be some
level of control over the distance function of the system. For instance, the
user of the program can specify for the output to be close in features to the
audio input, or this could be inverted and the resulting output could have
features that are the opposite of this. Some probabilistic systems could be
added, so that the chance of similar or dissimilar sonic output is controllable.&lt;/p&gt;

&lt;h3 id=&quot;technical-aspects-of-processing-real-time-data&quot;&gt;Technical Aspects of Processing Real-Time Data&lt;/h3&gt;
&lt;p&gt;The first thing that I wanted to achieve this week was to implement a lock-free
circular buffer/ring buffer class for my program. In real-time audio processing
applications, circular buffers are a very useful data structure, as they allow
the program to operate on fixed blocks of samples that are updated as new data
comes into the system.&lt;/p&gt;

&lt;p&gt;After doing a lot of research, I found a suitable implementation in the book
&lt;em&gt;Audio Anecdotes II: Tools, Tips, and Techniques for Digital Audio&lt;/em&gt;, and I am
now using it in my Synth class. Data from the microphone is continuously pushed
into the buffer, which can then be popped into auxillary buffers for processing.
At the moment, I am just directing the input straight to output in this manner.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Circular_Buffer_Animation.gif/400px-Circular_Buffer_Animation.gif&quot; alt=&quot;Ring Buffer&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;audio-analysis&quot;&gt;Audio Analysis&lt;/h3&gt;
&lt;p&gt;I have also been weighing up my options when approaching the audio analysis
aspects of my program. I had the decision to either use a library that provided
this functionality for me, or to implement it myself. I wanted to keep with my
plan of implementing most features at a low level, but I am also wary of
balancing my time spent and not taking too long implementing certain aspects of
the program. I will compare my findings when looking at several libraries for C++:&lt;/p&gt;

&lt;h4 id=&quot;essentia&quot;&gt;&lt;strong&gt;Essentia&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;This is a well-established library for doing audio analysis in C++, which
includes many of the most widely-used audio feature extraction algorithms. The
advantage of using it would be the time saved in not implementing such code
myself, and that it may possess less errors than self-written code. That being
said, integrating a large library’s functionality into my existing codebase also
brings with it some potential issues. Additionally, it would also be doing
much of the work for me, getting in the way of my learning how to implement such
processes myself. Although the end-result is of course important, one of my main
aims with this project is to get better at audio programming and understanding
signal processing, even if the process is more time consuming and error-prone.&lt;/p&gt;

&lt;h4 id=&quot;fftw3&quot;&gt;&lt;strong&gt;fftw3&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;This is a comparatively low-level library that primarily computes discrete
fourier transforms of signal vectors, through a highly efficient FFT algorithm.
This approach intrigued me as I wanted to gain more experience working with DFTs
in C++ and it would mean that although I wouldn’t have to create my own FFT
algorithm, I would have to implement much of the analysis code myself.&lt;/p&gt;

&lt;p&gt;In the end, I moved forwards with using the fftw3 library and planning the
creation of my own analysis functions, while keeping time constraints in mind. I
am aiming to give myself plenty of time and have the option to revert to
fallback approaches if the approach turns out to be too demanding. I can also
switch to using Essentia or another option later on this way, if needed.&lt;/p&gt;

&lt;h3 id=&quot;plans-moving-forwards&quot;&gt;Plans moving forwards&lt;/h3&gt;
&lt;p&gt;I have been working at quite a low level and the progress towards a working
prototype has been slow, due to the time that developing components from the
ground up can take. I want to stand by my approach, as it has been invaluable in
learning how to write audio/DSP code in C++, and I have been enjoying the
experience. That being said, it has made it difficult to reason about the aims
and success of my program, since I am still relatively far away from a working
prototype. With that in mind, I am aiming to move a bit faster with my
production schedule. I am now starting the audio analysis aspects of the program
and then working on the synthesis aspect shortly after.&lt;/p&gt;

&lt;p&gt;I am aiming to achieve a basic working command-line-only prototype within the
next several weeks, which I will flesh out further from there after assessing
its results.&lt;/p&gt;</content><author><name>Liam Wyllie</name></author><summary type="html">Thoughts on the Program’s Direction I have been thinking a lot about my aims with the program and my report, and the particular niche of musical concatenative synthesis I would like to explore. One aspect I would like to focus more strongly on, is the notion of working with real-time audio input. Thus, CATE would primarily be an experimental tool for generating variations of an audio source in real-time. Essentially, this will be a type of audio effect, where the incoming audio is transformed according to its analysed features.</summary></entry><entry><title type="html">Handling the Audio Database</title><link href="http://localhost:4000/2019/02/10/handling-audio-file-database.html" rel="alternate" type="text/html" title="Handling the Audio Database" /><published>2019-02-10T00:00:00+00:00</published><updated>2019-02-10T00:00:00+00:00</updated><id>http://localhost:4000/2019/02/10/handling-audio-file-database</id><content type="html" xml:base="http://localhost:4000/2019/02/10/handling-audio-file-database.html">&lt;h3 id=&quot;loading-a-directory-of-audio-files&quot;&gt;Loading a Directory of Audio Files&lt;/h3&gt;
&lt;p&gt;My first objective was to find a way of automatically loading every
audio file from a directory into memory in the program.&lt;/p&gt;

&lt;p&gt;After doing some research, I realised that this functionality only
exists natively in C++ with the C++17 standard, and I am using the
earlier C++14 standard in my project for compatibility
purposes. Therefore, I decided to use the filesystem header from the
boost library. This required installing the boost headers on my system
as I have not used them before when working with C++, and modifying my
CMakeLists file. After experimenting with the boost/filesystem
functions, I eventually implemented a way of getting the path of every
file deeper than a specified directory, using a recursive process:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A first, non-recursive function generates a list of all paths from a root directory.&lt;/li&gt;
  &lt;li&gt;The recursive function calls this function as its first statement.&lt;/li&gt;
  &lt;li&gt;It then iterates over that list.&lt;/li&gt;
  &lt;li&gt;If it encounters a directory path, the recursive function is called
again, passing the path as a parameter.&lt;/li&gt;
  &lt;li&gt;Otherwise, the recursive function adds the path to the final path list.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This has been working well for the particular directory of example
sounds I am using. One particular issue that might arise is if the
user has a directory filled with a mixture of audio files and
non-audio files, which could result in the contents of those non-audio
files being treated as sample data. I may have to test for this case
and implement a way of checking if a file is an audio file or not.&lt;/p&gt;

&lt;h3 id=&quot;pre-processing-the-audio-database&quot;&gt;Pre-processing the Audio Database&lt;/h3&gt;
&lt;p&gt;My first idea was to convert every audio file loaded to the same
sample rate (the standard 44.1kHz CD-quality rate). I chose to do
this, as I had read in some papers relating to doing audio analysis on
a database of sounds that this would result in more effective
results. It also made it easier to test audio files with my program,
as it means I don’t have to do any interpolation in the actual audio
processing function. I implemented a function for converting an audio
file to a new sample rate using the
&lt;a href=&quot;https://github.com/erikd/libsamplerate&quot;&gt;libsamplerate&lt;/a&gt;
library. Although I have been generally sticking to my philosophy of
avoiding C libraries and preferring native C++ code, I was only
implementing a very small function contained within my AudioBuffer
class, so I had no problems with it in this case. I found it
preferable to use this library than implementing my own sample rate
interpolation for a number of reasons:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Likely more efficient, faster, safer etc. than an implementation of my own.&lt;/li&gt;
  &lt;li&gt;Simplifies my own codebase by requiring less code.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;That being said, I did become interested in sample rate interpolation when looking into this, and it’s something that I would like to implement myself in the future.&lt;/p&gt;

&lt;h3 id=&quot;python-utilities&quot;&gt;Python Utilities&lt;/h3&gt;
&lt;p&gt;It has been my plan with this project to make use of the Numpy and
Matplotlib libraries for the Python programming language, to help with
the creation of C++ DSP code. This week I created some simple plotting
tools which allow me to quickly view the waveform or spectrum of a
given audio file. When I am developing the audio analysis functions,
it will be helpful to prototype them in Python and plot their results
before implementing them in C++.&lt;/p&gt;

&lt;h3 id=&quot;sdif-files&quot;&gt;SDIF Files&lt;/h3&gt;
&lt;p&gt;I began to research the SDIF file format more, thinking about how I
could use it in my program. I plan to implement the simplest parts
of its functionality to begin with, reading and writing files and
printing their contents in my program. When I progress to the audio
analysis aspects later, I will start to flesh out the SDIF code more.&lt;/p&gt;

&lt;h3 id=&quot;audio-buffer-segmentation&quot;&gt;Audio Buffer Segmentation&lt;/h3&gt;
&lt;p&gt;At this point I also wanted to experiment with splitting AudioBuffer
objects into sub-objects and playing them back with a modified version
of the Synth class. This was successful, and I was able to play back
short snippets of files from the database. It made me think about how
I will handle the segmentation process later on, as I realised that
pre-segmenting every audio file will likely be impractical and
inflexible. I looked back to how existing concatenative synthesis
implementations such as CataRT deal with this, and realised that
storing segmentation positions as numeric data may be the best
option. I can then do the actual segmentation in real-time in the
Synth::process function. Right now my understanding of this process
based on my research into concatenative synthesis is as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The audio database is analysed, and segmentation markers are
generated along with descriptors.&lt;/li&gt;
  &lt;li&gt;The information is stored in files to be recalled later. Each
segment may have its own row in the database along with columns of
features extracted from the audio analysis process.&lt;/li&gt;
  &lt;li&gt;During the synthesis stage, the user specifies target parameters,
and a combination of segments from the database are generated&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Once I begin working on the the analysis and synthesis stage of the
program, I will be able to experiment with the approach more. I plan
to first implement a non-realtime version which will allow me to
experiment with different target parameters through the command line
and create automated tests, which should make the process easier,
although I will also need a way of testing its real-time capabilities.&lt;/p&gt;

&lt;h3 id=&quot;moving-forwards&quot;&gt;Moving Forwards&lt;/h3&gt;
&lt;p&gt;At this point I have already partially completed some goals from weeks
3 and 4 of the production schedule. My goal at this point is to take
some time to improve the code that I already written along with the
rest of those production targets, while doing more reading and
planning of the analysis and synthesis stage.&lt;/p&gt;

&lt;p&gt;I have still not implemented audio input recording, which I will
definitely aim to achieve in the next two weeks. I also want to
implement a better command line interface with best practises for C++
CLI programs, expanding upon the simple interface I currently have,
which directly uses the argc and argv parameters inherited from C.&lt;/p&gt;</content><author><name>Liam Wyllie</name></author><summary type="html">Loading a Directory of Audio Files My first objective was to find a way of automatically loading every audio file from a directory into memory in the program.</summary></entry><entry><title type="html">Audio Programming in C++</title><link href="http://localhost:4000/2019/02/03/audio-programming-in-c++.html" rel="alternate" type="text/html" title="Audio Programming in C++" /><published>2019-02-03T00:00:00+00:00</published><updated>2019-02-03T00:00:00+00:00</updated><id>http://localhost:4000/2019/02/03/audio-programming-in-c++</id><content type="html" xml:base="http://localhost:4000/2019/02/03/audio-programming-in-c++.html">&lt;h3 id=&quot;starting-out&quot;&gt;Starting Out&lt;/h3&gt;

&lt;p&gt;My main goal when starting the project was to first establish which
libraries for working with audio in C++ would form the basis of the
program, and how I would design and structure the audio backend of the
program. I have found that when creating audio applications in C++,
choosing between C and C++ libraries is an important choice. Usually,
the option is to either use established, widely-used C libraries like
portaudio and libsndfile, and hide their C functions behind a C++
interface, or use native C++ libraries. I have noticed that certain
applications like SuperCollider opt for the former, however wrapping C
code in C++ can be time consuming and error-prone and should generally
be avoided when possible, as I have discussed with my supervisor. In
general, I think that it’s better to use the STL and modern C++
features, rather than using the backward-compatible C parts of
C++. This post recaps my experiences with exploring both options,
comparing them and expanding upon my motivations for ultimately
deciding upon using the native C++ bindings for the portaudio and
libsndfile libraries.&lt;/p&gt;

&lt;h3 id=&quot;reading-audio-file-data&quot;&gt;Reading Audio File Data&lt;/h3&gt;
&lt;p&gt;My first objective was dealing with uncompressed audio file formats
like WAV and AIFF; Implementing functionality for reading existing
files and storing their sample values in data structures, as well as
creating output files from sequences of samples.&lt;/p&gt;

&lt;p&gt;I initially used the C library libsndfile, which provides an easy way
of dealing with audio files without having to worry about issues like
file headers and endianness. This involved creating a wrapper class
that hid the C functions behind a C++ interface.&lt;/p&gt;

&lt;p&gt;One immediate issue I found was that due to C’s lack of templates and
function overloading, each return type for the sf_read functions has a
separate function. Working around this to create generic functions
would add unnecessary complexity, if I wanted my functions to work
with both floats and doubles for instance.&lt;/p&gt;

&lt;p&gt;Therefore, I eventually decided to use the C++ API instead, which
provides a SndfileHandle class with overloaded member functions,
fitting better into a C++ program.&lt;/p&gt;

&lt;h3 id=&quot;audio-io-api&quot;&gt;Audio I/O API&lt;/h3&gt;

&lt;p&gt;Prior to starting the project, I decided upon using the C PortAudio
library as I had experience using it in the past. I achieved good
results initially, using it in a procedural manner to play back audio
file data. When I wanted to adapt the code to a more templated
object-oriented C++ style that I intended the rest of my program to be
written in, I encountered some challenges that made me reassess my
options going forwards. Wrapping the functionality in C++ classes was
proving to be time consuming and requiring a lot of workarounds, so I
decided to use the C++ bindings for the library rather than create my
own wrappers. As well as being less time-consuming and allowing me to
focus more on writing the application itself, this has the advantage
of being better tested and more reliable due to being created by the
developers of the library.&lt;/p&gt;

&lt;h3 id=&quot;progress-summary&quot;&gt;Progress Summary&lt;/h3&gt;

&lt;p&gt;So far, the program is able to load audio file data into an
AudioBuffer class, and output them through the Synth::process member
function of the Synth class. Later, these components will be expanded
upon, so that the AudioBuffers can be split into smaller segmented
buffers that can be played back simultaneously.&lt;/p&gt;

&lt;p&gt;The program also has some basic multithreading capabilities so far
using the std::thread class from C++11, with the audio stream starting
on a separate thread while the main thread continues (as a while
(true) loop for now as user input isn’t needed yet).&lt;/p&gt;

&lt;p&gt;My plans moving forward are to add functionality for storing multiple
AudioBuffers in memory from a directory of audio files, in preparation
for the audio analysis part of my program. I may use the Boost library
for a portable way of interacting with the filesystem to achieve
this. I will also look ahead to how I can store the analysis data in
some kind of lightweight database, which may involve utilising the SDIF file format.&lt;/p&gt;</content><author><name>Liam Wyllie</name></author><summary type="html">Starting Out</summary></entry></feed>