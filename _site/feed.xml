<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2019-03-24T23:42:44+00:00</updated><id>/feed.xml</id><title type="html">CATE</title><subtitle>Project blog for the CATE program, updated with my progress and findings.</subtitle><author><name>Liam Wyllie</name></author><entry><title type="html">Expanding Upon Granular Synthesis</title><link href="/2019/03/24/expanding-upon-granular-synthesis.html" rel="alternate" type="text/html" title="Expanding Upon Granular Synthesis" /><published>2019-03-24T00:00:00+00:00</published><updated>2019-03-24T00:00:00+00:00</updated><id>/2019/03/24/expanding-upon-granular-synthesis</id><content type="html" xml:base="/2019/03/24/expanding-upon-granular-synthesis.html">&lt;h3 id=&quot;progress&quot;&gt;Progress&lt;/h3&gt;
&lt;p&gt;This week I expanded upon the granular synthesis code I had created in the
previous week. The grains are now properly derived from the audio files matched
from the analysis process. The synthesis process is also more efficient and
correctly implemented, using object pooling techniques. I also started working
on a command line interface for managing the analysis process.&lt;/p&gt;

&lt;h3 id=&quot;command-line-utility&quot;&gt;Command Line Utility&lt;/h3&gt;
&lt;p&gt;One aspect of the program I wanted to develop was a basic command line utility
to generate new analysis data from specified directories of audio files. I used
a C++ library called “args” to handle the implementation details, allowing my
program to have a sophisticated CLI tool with argument flags. So far, it can be
used in the following way:&lt;/p&gt;

&lt;p&gt;CATE_CLI -f 1024 -b 1024 analyse /directory/&lt;/p&gt;

&lt;p&gt;where -f and -b specify the frames per buffer and FFT bin size parameters of the
analysis system respectively. I included pre-processor directives in my C++ code
and tweaked my build system so that multiple binaries are built from the source
code, the CLI tool and the GUI application. I will probably also have a separate
application that just executes the audio callback without a GUI.&lt;/p&gt;

&lt;h3 id=&quot;granular-synthesis-improvements&quot;&gt;Granular Synthesis Improvements&lt;/h3&gt;
&lt;p&gt;I fixed some issues I had with my granular synthesis model, relating to grain
activation/deactivation. I achieved this by incorporating a sample counter
within the synthesis function of the Grain class, so that the objects are able
to keep track of when they should be inactive. Essentially, the synthesize
function within the Grain class first checks if there are any more samples to
generate, and returns a zero-amplitude sample if the resultant boolean value is
false. Otherwise, it synthesizes a new sample and decrements the sample counter.&lt;/p&gt;

&lt;p&gt;The grain creation function within the scheduler now iterates over the array of
grain objects until it finds an inactive grains, using that same is_active()
function. It then creates a new Grain object and immediately returns from the
function. This function is called on every interonset trigger.&lt;/p&gt;

&lt;p&gt;The program now sounds reminiscent of classic granular synthesis. One issue is
there is a lack of control over grain duration, which limits the possibilities
in the sound. This is due to the fact that the grain duration is implemented as
a constant, tied to the segmentation sizes of the audio analysis system. I have
decided to just have a max duration that’s tied to the maximum buffer size, and
then have the envelope duration be controllable.&lt;/p&gt;

&lt;h3 id=&quot;relating-the-synthesis-and-analysis-processes&quot;&gt;Relating the Synthesis and Analysis Processes&lt;/h3&gt;
&lt;p&gt;At the moment, the KNN function is called on each frame within the “frames per
buffer” loop in the audio callback function. It finds just one nearest neighbour
which is immediately used by the granular synthesis code. One approach may have
been to run this code on a separate thread, and have a queue of neighbours that
is consumed by the audio process. In the next week I will finalise this part of
the program, which will explore these options.&lt;/p&gt;

&lt;p&gt;The audio files in the database are provided to the Scheduler in a C++ std::map
structure, which is keyed by a string filename. The filename is obtained from
the KNN function along with a segmentation marker position, which allows
creating a new audio buffer at the given position in the respective audio file.
The duration of the buffer must be fixed, however control over duration could be
allowed through manipulating the amplitude envelope of grains.&lt;/p&gt;

&lt;h3 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h3&gt;
&lt;p&gt;For the next week, I aim to improve the analysis and synthesis process in
general, adding more audio features and making the synthesis process more
aesthetically pleasing, such as envelope following of the microphone input’s
amplitude to control the audio output’s amplitude. I am hoping to finish most of
the functionality of the program in the next week or two so that I can focus on
the interface, error handling and unit tests.&lt;/p&gt;</content><author><name>Liam Wyllie</name></author><summary type="html">Progress This week I expanded upon the granular synthesis code I had created in the previous week. The grains are now properly derived from the audio files matched from the analysis process. The synthesis process is also more efficient and correctly implemented, using object pooling techniques. I also started working on a command line interface for managing the analysis process.</summary></entry><entry><title type="html">Implementing Simple Granular Synthesis</title><link href="/2019/03/17/implementing-simple-granular-synthesis.html" rel="alternate" type="text/html" title="Implementing Simple Granular Synthesis" /><published>2019-03-17T00:00:00+00:00</published><updated>2019-03-17T00:00:00+00:00</updated><id>/2019/03/17/implementing-simple-granular-synthesis</id><content type="html" xml:base="/2019/03/17/implementing-simple-granular-synthesis.html">&lt;h3 id=&quot;progress&quot;&gt;Progress&lt;/h3&gt;
&lt;p&gt;My goal for this week and the following week was working on a granular synthesis implementation. For the first week, I
 have aimed to just get a simple system working that has the core functionality needed going forwards. Then I plan to
  integrate it with my audio analysis code, so that the grains are derived from the matches found by the KNN search.&lt;/p&gt;

&lt;h3 id=&quot;establishing-the-granular-synthesis-model&quot;&gt;Establishing the Granular Synthesis Model&lt;/h3&gt;
&lt;p&gt;I wanted to find a system for implementing granular synthesis in C++ that would have robust foundations and be 
modular/expandable. 
After doing a 
bit of research, I found the following &lt;a href=&quot;http://www.rossbencina.com/static/code/granular-synthesis/BencinaAudioAnecdotes310801.pdf&quot;&gt;article&lt;/a&gt; written by Ross Bencina, who also designed the PortAudio library I am 
using in my program. It seemed to be an ideal starting point, so I set out to understand it and implement the class 
structure proposed within it. In the article, a model for granular synthesis is broken down into different objects, 
each of which I will write briefly about:&lt;/p&gt;

&lt;h4 id=&quot;source&quot;&gt;Source&lt;/h4&gt;
&lt;p&gt;The audio source used in the granulation, which can be of the following types:&lt;/p&gt;
&lt;h5 id=&quot;tapped-delay-line&quot;&gt;Tapped Delay Line&lt;/h5&gt;
&lt;p&gt;Sample data is stored from a real-time stream as a delay line. This is
particularly useful for granulators that operate as an audio effect. It can be
seen as the most complicated type of granular synthesis to implement, because
the delay line must be maintained and variable rate delay taps have to be
implemented for each grain.&lt;/p&gt;

&lt;h5 id=&quot;synthetic-grains&quot;&gt;Synthetic Grains&lt;/h5&gt;
&lt;p&gt;Each grain is synthesized through synthesis techniques such
as FM synthesis. Potentially simple to implement, because state doesn’t need to
be shared between grains.&lt;/p&gt;

&lt;h5 id=&quot;stored-samples&quot;&gt;Stored Samples&lt;/h5&gt;
&lt;p&gt;Sample values for grains are stored in wavetables, which can be used to process
pre-generated waveforms or sample data from audio files.&lt;/p&gt;

&lt;p&gt;As the goal of CATE is to resynthesize the audio from audio files, stored
samples will be the type of source.&lt;/p&gt;

&lt;h4 id=&quot;envelope&quot;&gt;Envelope&lt;/h4&gt;
&lt;p&gt;Generates sample values for an amplitude envelope. Types of envelope include simple parabolic envelopes, 
trapezoidal envelopes, or raised cosine bell envelopes. Grain envelopes can introduce spectral artifacts.
 Trapezoidal envelopes introduce the most spectral distortion due to discontinuities at the edges. An 
 alternative is raised cosine bell envelopes which are better in this regard, at the expense of greater processing 
 time.&lt;/p&gt;

&lt;p&gt;There is a general time/space tradeoff with implementing amplitude envelopes, where you
 may choose to either compute the coefficients on demand or use a pre-computed
 wavetable. Bencina states that modern processors allow for a more efficient
 implementation when not using stored tables, due to the memory access overhead
 of accessing stored tables.&lt;/p&gt;

&lt;p&gt;I chose to implement a Parabolic envelope for now due to its relative
 simplicity, which I can always swap out later for a raised cosine bell envelope,
 which allows for more control over the envelope parameters.&lt;/p&gt;

&lt;h4 id=&quot;grain&quot;&gt;Grain&lt;/h4&gt;
&lt;p&gt;Combines an Envelope and Source object to form an overall model of a grain in granular synthesis. The
 source sample is multiplied by the envelope sample when synthesized.&lt;/p&gt;

&lt;h4 id=&quot;scheduler&quot;&gt;Scheduler&lt;/h4&gt;
&lt;p&gt;Generates time periods between grains and handles activating them. It also manages mixing and outputting active 
grains. A simple counting system generates a new &lt;a href=&quot;https://en.wikipedia.org/wiki/Time_point#Interonset_interval&quot;&gt;interonset&lt;/a&gt; 
time whenever the sample counter is equal to 0, and activates the 
next grain. Then, that sample counter is decremented. When it hits 0 again, the cycle continues and the next grain is
 activated. The interonset time can be generated with a simple function that uses a random number generator. I used 
functionality from the random header in C++ to get uniform distribution random numbers.&lt;/p&gt;

&lt;h4 id=&quot;granulator&quot;&gt;Granulator&lt;/h4&gt;
&lt;p&gt;The top-level class that contains the scheduler and any other necessary high-level functionality for 
using the synth. This is the class that will have a spawned object in the AudioProcess class of CATE, while the 
previous classes will all be hidden.&lt;/p&gt;

&lt;h3 id=&quot;challenges-to-overcome-going-forwards&quot;&gt;Challenges to Overcome Going Forwards&lt;/h3&gt;
&lt;p&gt;After having implemented some basic granular synthesis, I feel that I have now
come to one of the more challenging aspects of implementing a granular system
that works with a large set of audio files, where the sliced segments within the files to be used as grains is 
constantly changing. Currently, I know that I have the following functionality to work with:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A granular synthesis system that requires each grain to be provided with a source wavetable array.&lt;/li&gt;
  &lt;li&gt;An audio analysis pipeline that relates lists of analysis features and segmentation positions with audio files&lt;/li&gt;
  &lt;li&gt;A real-time analysis system that outputs the same features that are used in the stored analysis data&lt;/li&gt;
  &lt;li&gt;A k-nearest neighbours function that returns the N nearest indices from the list of segmentation markers that 
correspond to the 
current block of input from the microphone&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My task now is to tie these systems together, so that the segments extracted
from the audio files are granulated according to their features that are matched
with the microphone input.&lt;/p&gt;

&lt;h3 id=&quot;current-progress-check-and-future-plans&quot;&gt;Current Progress Check and Future Plans&lt;/h3&gt;
&lt;p&gt;I am generally satisfied with my progress this week, as the most difficult aspect was approaching a topic like 
granular synthesis and understanding how to implement it in C++, while I am still new to low-level audio programming.
 I’m happy with the article written by Bencina that I found, as it improved my understanding of granular synthesis 
 and helped me get started on implementing it. I plan to spend the next week improving upon what I have, and properly
  combining it with the analysis aspects of the program. I may look into the concept of &lt;a href=&quot;https://en.wikipedia.org/wiki/Object_pool_pattern&quot;&gt;object pooling&lt;/a&gt;, which is a 
  design pattern that specifies a set of pre-initialised objects that can reused, rather than destroying objects and 
  allocating new ones. Pooled objects can be recalled in a determinant amount of time as opposed to allocated 
  objects, which may take an indeterminant amount of time (an undesirable property in audio programming).&lt;/p&gt;</content><author><name>Liam Wyllie</name></author><summary type="html">Progress My goal for this week and the following week was working on a granular synthesis implementation. For the first week, I have aimed to just get a simple system working that has the core functionality needed going forwards. Then I plan to integrate it with my audio analysis code, so that the grains are derived from the matches found by the KNN search.</summary></entry><entry><title type="html">Sliding Window Analysis and K-d Tree</title><link href="/2019/03/10/sliding-window-analysis-and-kd-tree.html" rel="alternate" type="text/html" title="Sliding Window Analysis and K-d Tree" /><published>2019-03-10T00:00:00+00:00</published><updated>2019-03-10T00:00:00+00:00</updated><id>/2019/03/10/sliding-window-analysis-and-kd-tree</id><content type="html" xml:base="/2019/03/10/sliding-window-analysis-and-kd-tree.html">&lt;h3 id=&quot;progress&quot;&gt;Progress&lt;/h3&gt;
&lt;p&gt;This week I have been building upon the audio analysis code implemented
previously. I have added functionality to perform that analysis on fixed audio
files as well as on the live input that was already present. I also added the
K-d Tree implementation to perform nearest neighbour searches, using the
&lt;a href=&quot;https://github.com/jlblancoc/nanoflann&quot;&gt;nanoflann&lt;/a&gt; library. I also fixed some
critical bugs with my program, as well as simplifying the codebase.&lt;/p&gt;

&lt;h3 id=&quot;code-simplifications&quot;&gt;Code Simplifications&lt;/h3&gt;
&lt;p&gt;Previously, I had a class design for audio data which was meant to encompass
audio data loaded from files as well as sample data in general, for example
buffers created from mathematical functions. I wasn’t happy with how I had
implemented this, where the sample data itself was encapsulated in a private
member variable, with public member functions for operations such as file
reading. The problem with this was that I had to make heavy usage of getter
functions to access the data in other classes, which defeated the purpose of it
for me. Therefore I decided to just work in a simpler way, passing std::vectors
of sample data around instead, and having specific, simple classes such as an
AudioFile class that just had the vector with public access. I used a typedef to
alias the vector to the name AudioBuffer to make it more symbolic and easier to
work with, and separate its meaning from the specific std::vector container. I
also removed a few functions from my program that I am not using currently,
which I can always add again later if necessary.&lt;/p&gt;

&lt;h3 id=&quot;fixing-a-painful-memory-bug&quot;&gt;Fixing a Painful Memory Bug&lt;/h3&gt;
&lt;p&gt;Recently I have been having some problems with my program relating to memory
that I was struggling to debug due to the nature of such issues. My program was
having issues with segmentation faults and other memory-related errors. I
eventually solved it by taking an approach where I would comment out all but the
most critical parts of my program and recompile, until the issue had gone away.
Then, I would continuously uncomment, recompile and test in a cycle, waiting to
find the combination of code that resulted in the memory errors. While going
through this process, I removed the spectrogram plotting code, as I no longer
planned to use it, and I also suspected it may be part of the problem. This
suspicion proved warranted, as my program is now free of those memory errors,
which is extremely good going forwards.&lt;/p&gt;

&lt;h3 id=&quot;sliding-window-analysis-of-audio-files&quot;&gt;Sliding Window Analysis of Audio Files&lt;/h3&gt;
&lt;p&gt;Previously, I had implemented FFT routines as well as functions for calculating
two different spectral features: spectal centroid and spectral flatness, which I
had successfully implemented in my audio callback function.&lt;/p&gt;

&lt;p&gt;My next goal was to implement this same functionality, but on frames of audio
files. The frame size should be equal to the frames per buffer variable I was
using in my audio callback function, so that the extracted data was equivalent
in meaning. I created functions that iterated over the loaded audio files,
moving a window of &lt;frame_size&gt; frames, and passing those subvectors to my FFT
and spectral feature pipeline. Then, this data is able to be stored in the
analysis files, along with the marker positions and filenames.&lt;/frame_size&gt;&lt;/p&gt;

&lt;h3 id=&quot;k-d-tree-implementation&quot;&gt;K-d Tree Implementation&lt;/h3&gt;
&lt;p&gt;A K-d tree is a data structure that partitions data in such a way that you can
perform k-nearest neighbour searches of the data much more efficiently. The
search time complexity when using a K-d tree is an average case of O(log n),
whereas regular KNN searches are at best O(nd) from the research I have done,
which is significantly slower. One tradeoff is there is some time needed to
build the tree, but this is not nearly as important as the time spent doing the
search itself, as the search will be taking place in the more critical real-time
audio part of my code.&lt;/p&gt;

&lt;p&gt;As mentioned previously, I used the nanoflann library for this. I had done some
research, and it seemed like the perfect option, as one of its main priorities
is speed and efficiency, which is important for real-time concerns of my
program. My utilisation of it required some particular details. Notably, the
library is designed in such a way that means you may only have one instance of
the K-d tree object in place in your program at a time, as its copy constructor
and assignment operators are deleted. Therefore, I implemented the object in the
main function of my program. Then, a reference to that object is passed to
functionality that needs it. Essentially, you create a primitive data structure
where your data is stored as points. You create a std::vector of these point
objects, which is used by the K-d tree object to contruct the tree index. Then,
the workflow is as simple as calling one function that takes the following
arguments:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;An array of input points&lt;/li&gt;
  &lt;li&gt;The number of search results to return&lt;/li&gt;
  &lt;li&gt;A std::vector to store the return indices of your container of data points found by the search&lt;/li&gt;
  &lt;li&gt;Another std::vector to store the actual distances of those indices from the input points&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I have added a call to it within my audio callback function, and it is operating
smoothly so far. I am passing it the centroid and flatness values generated from
the microphone input, and it is outputting the files and markers in the audio
file database that are closest in distance to the current values. My plan is to
use this information to drive a granular synthesis process, where the selected
grains are determined by the file IDs and segmentation markers.&lt;/p&gt;

&lt;h3 id=&quot;moving-onwards&quot;&gt;Moving Onwards&lt;/h3&gt;
&lt;p&gt;In my production schedule, the next two weeks were allocated towards the
granular synthesis implementation. I am happy about this, as it means that I
have been following and meeting my production schedule perfectly. I plan to take
my time and first implement a very simple synthesis system. Then I can add more
complicated features later. I’m particularly interested in how I can break down
the granular/concatenative synthesis process into different constituent parts,
in a flexible way. I will be looking to different code examples to help me
achieve the best results possible.&lt;/p&gt;</content><author><name>Liam Wyllie</name></author><summary type="html">Progress This week I have been building upon the audio analysis code implemented previously. I have added functionality to perform that analysis on fixed audio files as well as on the live input that was already present. I also added the K-d Tree implementation to perform nearest neighbour searches, using the nanoflann library. I also fixed some critical bugs with my program, as well as simplifying the codebase.</summary></entry><entry><title type="html">Analysis Functionality and Data Storage</title><link href="/2019/03/03/analysis-functionality-and-data-storage.html" rel="alternate" type="text/html" title="Analysis Functionality and Data Storage" /><published>2019-03-03T00:00:00+00:00</published><updated>2019-03-03T00:00:00+00:00</updated><id>/2019/03/03/analysis-functionality-and-data-storage</id><content type="html" xml:base="/2019/03/03/analysis-functionality-and-data-storage.html">&lt;h3 id=&quot;progress&quot;&gt;Progress&lt;/h3&gt;
&lt;p&gt;I have been implementing functionality for audio analysis and storage for the program. I am on schedule with this and will continue working on it next week as well.&lt;/p&gt;

&lt;p&gt;This week’s additions are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Visualisation of the magnitude spectrum&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Implementing spectral analysis functions&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Implementing persistent storage of analysis data in JSON format&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;General Changes to Code Design&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;visualisation-of-the-magnitude-spectrum&quot;&gt;Visualisation of the Magnitude Spectrum&lt;/h3&gt;
&lt;p&gt;I wanted a way of verifying the workings of my magnitude spectrum computation with visual feedback. I used a &lt;a href=&quot;http://www.martin-kumm.de/wiki/doku.php?id=05Misc:A_Template_for_Audio_DSP_Applications&quot;&gt;code example&lt;/a&gt; found online as a guideline when working on this. I will likely have a very different GUI by the completion of my program, but for now it has been helpful for this purpose, as well as teaching me more about Qt and the Qt plotting library. I may also add some extra plotting of my audio features, as a way of verifying their results easily as well.&lt;/p&gt;

&lt;h3 id=&quot;implementing-spectral-analysis-functions&quot;&gt;Implementing Spectral Analysis Functions&lt;/h3&gt;
&lt;p&gt;This week, I initially spent some time working with the &lt;a href=&quot;https://essentia.upf.edu/&quot;&gt;Essentia&lt;/a&gt; library, discovering if it would be suitable for my needs. In the end, I decided not to use it. All of the provided examples of its usage are non-real time utilisations, where analysis data is written to a file. It is important for my program to have real-time analysis. I found there to be very little guidance on doing this with Essentia, with the one example provided using an out of date version of Essentia, with numerous functions used within it that no longer exist in the library.&lt;/p&gt;

&lt;p&gt;Therefore, I decided to implement my own audio analysis functionality, which was made easier by the fact that I have already worked on the FFT code. Leading on from that, I chose to implement two features in particular: spectral centroid and spectral flatness. Some reasons for this are as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;They are relatively easy to calculate if you already have a magnitude spectrum computed, which I had functionality for with my previous week’s work&lt;/li&gt;
  &lt;li&gt;Compared to many other features, they are quite perceptually useful for a broad range of sounds that aren’t necessarily tonal or rhythmic, which is ideal for the audio sources my program is intended to be making use of. Centroids are linked to brightness in timbre, and flatness is a noisiness ratio where 1.0 is white noise and 0.0 is sinusoidal.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I am focusing on implementing just two features right now, and then moving on to working on the nearest neighbours search functionality. Once that is functioning correctly, I will look to add more analysis functions.&lt;/p&gt;

&lt;p&gt;Currently, the analysis code is structured in a way that there is a base “Feature” class that contains data like the sample rate of the system, which other feature classes can inherit from. I may simplify this later if the polymorphism-based approach seems unnecessary, but for now it seems like a logical way to do it.&lt;/p&gt;

&lt;h3 id=&quot;implementing-persistent-storage-of-analysis-data-in-json-format&quot;&gt;Implementing Persistent Storage of Analysis Data in JSON Format&lt;/h3&gt;
&lt;p&gt;It was important to add a way of persistently storing audio file paths and their associated analysis data arrays in some kind of data format.&lt;/p&gt;

&lt;p&gt;My choices were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A database system / SQL&lt;/li&gt;
  &lt;li&gt;Creating and parsing plain text files&lt;/li&gt;
  &lt;li&gt;Using a data format like XML or JSON&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I went with JSON for a number of reasons:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;It’s very easy to work with&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It’s a standard, portable format that is widely used and well-understood&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There is good functionality for working with it in C++ (through a third-party
library)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One potential drawback could be that the file size may grow large with a lot of
audio files and associated analysis data. The program likely won’t be working
with many audio files at once however, and JSON is designed to store data in
much larger quantities than my program will be using.&lt;/p&gt;

&lt;p&gt;I used the &lt;a href=&quot;https://github.com/nlohmann/json&quot;&gt;following library&lt;/a&gt; to get the functionality required.&lt;/p&gt;

&lt;h3 id=&quot;changes-to-overall-code-design&quot;&gt;Changes to Overall Code Design&lt;/h3&gt;
&lt;p&gt;One change was to change my usages of unsigned integer types with the simple “int” signed integer type. &lt;a href=&quot;http://soundsoftware.ac.uk/c-pitfall-unsigned.html&quot;&gt;I learned about some bugs that unsigned integers can result in&lt;/a&gt;. Using ints for all integer types also simplifies the code and makes it more readable.&lt;/p&gt;

&lt;p&gt;I also made some other simple type-related revisions, such as using double for sample rate variables, both for the reason that it is consistent with the libraries I am using, and that it makes the values easier to work with when doing calculations.&lt;/p&gt;

&lt;p&gt;I also made more usage of the “auto” keyword in function bodies, which is considered good practice in modern C++. I plan to aim to write most of my code in this way, with the exception of some of audio code where I may have to focus more on efficiency and a more low-level way of doing things if necessary.&lt;/p&gt;

&lt;h3 id=&quot;moving-onwards&quot;&gt;Moving Onwards&lt;/h3&gt;
&lt;p&gt;My immediate plans moving forwards are as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Performing a sliding-window analysis of audio files, and storing the resulting data in the JSON data file&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Building Kd-tree from data and using KNN algorithm to find closest audio unit to current analysis frame. I plan to use the &lt;a href=&quot;https://github.com/jlblancoc/nanoflann&quot;&gt;following library&lt;/a&gt; for this&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Adding some more audio features, particularly spectral ones&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Liam Wyllie</name></author><summary type="html">Progress I have been implementing functionality for audio analysis and storage for the program. I am on schedule with this and will continue working on it next week as well.</summary></entry><entry><title type="html">Discrete Fourier Transforms and Starting on Qt GUI</title><link href="/2019/02/24/discrete-fourier-transforms-and-starting-qt-gui.html" rel="alternate" type="text/html" title="Discrete Fourier Transforms and Starting on Qt GUI" /><published>2019-02-24T00:00:00+00:00</published><updated>2019-02-24T00:00:00+00:00</updated><id>/2019/02/24/discrete-fourier-transforms-and-starting-qt-gui</id><content type="html" xml:base="/2019/02/24/discrete-fourier-transforms-and-starting-qt-gui.html">&lt;h3 id=&quot;this-week&quot;&gt;This Week&lt;/h3&gt;
&lt;p&gt;This week, I’ve been working on the following parts of my program:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Discrete Fourier Transforms&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Starting on the Qt GUI&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;More Work on Python Utilities&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Creating a More Generalised Audio Buffer Class&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;implementing-discrete-fourier-transforms&quot;&gt;Implementing Discrete Fourier Transforms&lt;/h3&gt;
&lt;p&gt;For spectral audio analysis and visualisation, having some way of converting my
audio sample data to the frequency domain will be essential.&lt;/p&gt;

&lt;p&gt;My requirements for a FFT library were that it should be a well-established,
standard library used by high-profile open source programs, and that it should
only offer functionality for doing discrete fourier transforms and not other
unnecessary DSP functions that I was already implementing myself. The only
library that really met this criteria was &lt;a href=&quot;http://www.fftw.org/&quot;&gt;fftw&lt;/a&gt;. This is
a very significant library within open source/free-as-in-freedom software,
having been used by a lot of audio and signal processing applications in
general. I felt that it was a good opportunity for me to learn how to use it.
The only downside was that it’s natively a C library rather than a C++ library,
but I could not find any natively C++ alternatives.&lt;/p&gt;

&lt;p&gt;The library can be quite unforgiving to use, with lots of potential for
segmentation faults and other issues with array access due to how memory works
in C. After some experimentation, I eventually got a good system working. The
fftw library works in the following way:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Allocate memory for input and output arrays&lt;/li&gt;
  &lt;li&gt;Create a “plan” that defines the operations carried out, e.g
&lt;em&gt;complex-to-complex&lt;/em&gt; or &lt;em&gt;real-to-complex&lt;/em&gt;, as well as specifying the arrays
created in the last step.&lt;/li&gt;
  &lt;li&gt;Call the fftw_execute function to get the output spectrum array, which can be
called as many times as needed (e.g, in a real-time loop).&lt;/li&gt;
  &lt;li&gt;Deallocate memory and call fftw_destroy_plan when done (e.g, when audio stream is finished).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I used the &lt;a href=&quot;http://www.fftw.org/fftw3_doc/One_002dDimensional-DFTs-of-Real-Data.html&quot;&gt;one-dimensional
real-to-complex&lt;/a&gt;
plan, which converts a one-dimensional input of floating point numbers to an
output array of complex numbers. The specified input array is &lt;em&gt;n&lt;/em&gt; real numbers,
and the output is &lt;em&gt;n/2+1&lt;/em&gt; complex numbers, which means the input array needs to
be filled with &lt;em&gt;N/2+1&lt;/em&gt; elements and then padded with zeros. Additionally, the
array of data should be windowed, to avoid discontinuities at the starts and
ends. I just included an application of the Hann function within the function
that fills the data buffer, which is defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w(n) = \frac{1} {2} \left(1 - cos\left(\frac {2\pi n} {N - 1}\right)\right)&lt;/script&gt;

&lt;p&gt;where &lt;em&gt;n&lt;/em&gt; is the sample index and &lt;em&gt;N&lt;/em&gt; is the total number of samples.&lt;/p&gt;

&lt;p&gt;Generally when using DFT data, the magnitude of the resultant complex output is taken. This can be calculated as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sqrt{r^2 + i^2}&lt;/script&gt;

&lt;p&gt;where &lt;em&gt;r&lt;/em&gt; and &lt;em&gt;i&lt;/em&gt; are the real and imaginary parts of a complex number
respectively. When using std::complex in C++, you can just use the std::abs
function from the standard library to get the magnitude, making for a more
elegant implementation.&lt;/p&gt;

&lt;p&gt;To use my spectrum array in a meaningful way, I will need to convert the
indices to their respective frequencies. The following formula can be used for
converting bin index to frequency:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{n  \frac{Fs} {2}} {N}&lt;/script&gt;

&lt;p&gt;where &lt;em&gt;n&lt;/em&gt; is the bin index, &lt;em&gt;Fs&lt;/em&gt; is the sampling rate and &lt;em&gt;N&lt;/em&gt; is the total number
of bins. Hence, the 512th bin represents the magnitude of the frequency 11025Hz,
given a bin size of 1024 and a sample rate of 44100.&lt;/p&gt;

&lt;h3 id=&quot;starting-on-the-qt-gui&quot;&gt;Starting on the Qt GUI&lt;/h3&gt;
&lt;p&gt;I decided to commit to using Qt in my project and start working on it ahead of
schedule. This is mostly because I’ve realised that the backend of the code will
need to be redesigned quite a lot around the general practises of Qt
applications, such as the idea of QObjects. My reasons for using Qt are mostly
that it’s a well-established GUI library/framework that has been used by a lot
of applications, including notable audio software like
&lt;a href=&quot;https://supercollider.github.io&quot;&gt;SuperCollider&lt;/a&gt; and
&lt;a href=&quot;https://www.sonicvisualiser.org/&quot;&gt;SonicVisualiser&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I had to alter my CMake configuration to get Qt working, using the &lt;a href=&quot;https://github.com/euler0/mini-cmake-qt&quot;&gt;following
github repo&lt;/a&gt; as a guideline.&lt;/p&gt;

&lt;p&gt;I then aimed to get a program working where my audio process is running in a
background with a Qt window interface in the foreground. I started with
&lt;a href=&quot;https://github.com/fabienpn/simple-qt-thread-example&quot;&gt;the following example from a github
repo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I also found &lt;a href=&quot;http://www.martin-kumm.de/wiki/doku.php?id=05Misc:A_Template_for_Audio_DSP_Applications&quot;&gt;this
example&lt;/a&gt;
of combining PortAudio, Qt and FFTW which was perfect for what I’m trying to do.&lt;/p&gt;

&lt;p&gt;Learning from those examples, I now have the Qt graphics framework implemented
in my program. The program now launches as a window with buttons for starting
and stopping the audio engine. The downside of this, is that I was forced to
revert my audio code to use the C API of PortAudio rather than the C++ version.
Integrating the C++ PortAudio code with Qt was proving troublesome, and it was
considerably easier to wrap the low-level C code instead. I did want to avoid
this, but after reading some discussions in the PortAudio mailing list, this
seems to be what is generally done by people working with PortAudio in C++
anyway.&lt;/p&gt;

&lt;p&gt;As for Qt, I will leave the actual bulk of the GUI design until much later, but
I want to get Qt integrated into my program as soon as possible. Hopefully
within the next month or so I will have a working prototype of the final system.
Then, I can improve upon it while working on the GUI. The final GUI will likely
be fairly simple, as I am new to Qt and it could be very time consuming to
create something complex in it. Additionally, the idea for my program is that it
should have a minimal interface and not require a lot of user input anyway!&lt;/p&gt;

&lt;h3 id=&quot;creating-a-more-generalised-audio-buffer-class&quot;&gt;Creating a More Generalised Audio Buffer Class&lt;/h3&gt;
&lt;p&gt;I have also been thinking about the overall design of my program and its data
flow. As I’ve mostly been working on audio input, ring buffer and FFT code
recently, the earlier code I had written to read and write data from sound files
has become somewhat incompatible with the new code. My idea now is to have
interoperability between my classes that work with sample data. RingBuffer, FFT,
and AudioBuffer objects filled with data from audio files should be able to
communicate and share data easily. Therefore, I have looked into making my
AudioBuffer class a base class for the others and using inheritance to achieve
this.&lt;/p&gt;

&lt;h3 id=&quot;more-python-utilities&quot;&gt;More Python Utilities&lt;/h3&gt;
&lt;p&gt;I also did a bit more work on the simple Python programs in the scripts
directory. I plan to compare the results of my C++ program’s outputs for things
like magnitude spectrum and analysis data. With libraries like
&lt;a href=&quot;https://librosa.github.io/librosa/&quot;&gt;librosa&lt;/a&gt;, I can check what the output
should look like for these processes very quickly and easily. My other
motivation for doing this, is related to the overall ethos I have with this in
project in thinking in the long-term as well as about the project itself. I
think that improving my proficiency at using libraries with librosa and Python
prototyping in general will be a useful skill for me in the future.&lt;/p&gt;

&lt;h3 id=&quot;moving-onwards&quot;&gt;Moving Onwards&lt;/h3&gt;
&lt;p&gt;My plan for the next week is to experiment with creating my own audio analysis
system with fftw and other low-level libraries. As a fallback option, I have
installed the Essentia library on my system and I will use it if this becomes
too much work. I am also thinking ahead to some of the next steps I will need to
take, such as the nearest-neighbours search of the parameter-space once I have
my analysis functionality sorted out. I have covered the K-Nearest neighbours
algorithm in other classes, where I learned that the K-d tree data structure
allows for a more efficient implementation of the algorithm, which will likely
be necessary so I can perform the comparisons efficiently in the audio loop.&lt;/p&gt;

&lt;p&gt;Additionally, I want to implement better error handling with my PortAudio code
and clean it up in general. I will refrain from doing a lot of GUI work right
now, although I might add some kind of text output in the main window for
debugging pourposes.&lt;/p&gt;</content><author><name>Liam Wyllie</name></author><summary type="html">This Week This week, I’ve been working on the following parts of my program:</summary></entry><entry><title type="html">Moving Onwards: Real-time Audio Analysis</title><link href="/2019/02/17/going-forwards-with-real-time-audio-analysis.html" rel="alternate" type="text/html" title="Moving Onwards: Real-time Audio Analysis" /><published>2019-02-17T00:00:00+00:00</published><updated>2019-02-17T00:00:00+00:00</updated><id>/2019/02/17/going-forwards-with-real-time-audio-analysis</id><content type="html" xml:base="/2019/02/17/going-forwards-with-real-time-audio-analysis.html">&lt;h3 id=&quot;thoughts-on-the-programs-direction&quot;&gt;Thoughts on the Program’s Direction&lt;/h3&gt;
&lt;p&gt;I have been thinking a lot about my aims with the program and my report, and the
particular niche of musical concatenative synthesis I would like to explore. One
aspect I would like to focus more strongly on, is the notion of working with
real-time audio input. Thus, CATE would primarily be an experimental tool for
generating variations of an audio source in real-time. Essentially, this will be
a type of audio effect, where the incoming audio is transformed according to its
analysed features.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;CataRT&lt;/em&gt; offers this functionality, but overall it’s a more general-purpose
realisation of corpus-based concatenative synthesis in a musical context. I
would aim to focus on this aspect of the technique and see how it can be
expanded upon and made more usable by electronic music practitioners such as
myself. One aspect that would be important to me, would be for the system to
easily “playable” by a solo performer/composer. Thus, the control of the system
should require little intervention after an initial configuration process. It
may be useful to do more research into current studies in improvisation-based
music software systems. The paper &lt;em&gt;Electro/Acoustic Improvisation and Deeply
Listening Machines&lt;/em&gt; touches upon some ideas related to this.&lt;/p&gt;

&lt;p&gt;Ideally, the relationship between the source sound and the new sound
concatenated from the corpus should be identifiable. As discussed with my
supervisor, one potentially interesting approach could be for there to be some
level of control over the distance function of the system. For instance, the
user of the program can specify for the output to be close in features to the
audio input, or this could be inverted and the resulting output could have
features that are the opposite of this. Some probabilistic systems could be
added, so that the chance of similar or dissimilar sonic output is controllable.&lt;/p&gt;

&lt;h3 id=&quot;technical-aspects-of-processing-real-time-data&quot;&gt;Technical Aspects of Processing Real-Time Data&lt;/h3&gt;
&lt;p&gt;The first thing that I wanted to achieve this week was to implement a lock-free
circular buffer/ring buffer class for my program. In real-time audio processing
applications, circular buffers are a very useful data structure, as they allow
the program to operate on fixed blocks of samples that are updated as new data
comes into the system.&lt;/p&gt;

&lt;p&gt;After doing a lot of research, I found a suitable implementation in the book
&lt;em&gt;Audio Anecdotes II: Tools, Tips, and Techniques for Digital Audio&lt;/em&gt;, and I am
now using it in my Synth class. Data from the microphone is continuously pushed
into the buffer, which can then be popped into auxillary buffers for processing.
At the moment, I am just directing the input straight to output in this manner.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Circular_Buffer_Animation.gif/400px-Circular_Buffer_Animation.gif&quot; alt=&quot;Ring Buffer&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;audio-analysis&quot;&gt;Audio Analysis&lt;/h3&gt;
&lt;p&gt;I have also been weighing up my options when approaching the audio analysis
aspects of my program. I had the decision to either use a library that provided
this functionality for me, or to implement it myself. I wanted to keep with my
plan of implementing most features at a low level, but I am also wary of
balancing my time spent and not taking too long implementing certain aspects of
the program. I will compare my findings when looking at several libraries for C++:&lt;/p&gt;

&lt;h4 id=&quot;essentia&quot;&gt;&lt;strong&gt;Essentia&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;This is a well-established library for doing audio analysis in C++, which
includes many of the most widely-used audio feature extraction algorithms. The
advantage of using it would be the time saved in not implementing such code
myself, and that it may possess less errors than self-written code. That being
said, integrating a large library’s functionality into my existing codebase also
brings with it some potential issues. Additionally, it would also be doing
much of the work for me, getting in the way of my learning how to implement such
processes myself. Although the end-result is of course important, one of my main
aims with this project is to get better at audio programming and understanding
signal processing, even if the process is more time consuming and error-prone.&lt;/p&gt;

&lt;h4 id=&quot;fftw3&quot;&gt;&lt;strong&gt;fftw3&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;This is a comparatively low-level library that primarily computes discrete
fourier transforms of signal vectors, through a highly efficient FFT algorithm.
This approach intrigued me as I wanted to gain more experience working with DFTs
in C++ and it would mean that although I wouldn’t have to create my own FFT
algorithm, I would have to implement much of the analysis code myself.&lt;/p&gt;

&lt;p&gt;In the end, I moved forwards with using the fftw3 library and planning the
creation of my own analysis functions, while keeping time constraints in mind. I
am aiming to give myself plenty of time and have the option to revert to
fallback approaches if the approach turns out to be too demanding. I can also
switch to using Essentia or another option later on this way, if needed.&lt;/p&gt;

&lt;h3 id=&quot;plans-moving-forwards&quot;&gt;Plans moving forwards&lt;/h3&gt;
&lt;p&gt;I have been working at quite a low level and the progress towards a working
prototype has been slow, due to the time that developing components from the
ground up can take. I want to stand by my approach, as it has been invaluable in
learning how to write audio/DSP code in C++, and I have been enjoying the
experience. That being said, it has made it difficult to reason about the aims
and success of my program, since I am still relatively far away from a working
prototype. With that in mind, I am aiming to move a bit faster with my
production schedule. I am now starting the audio analysis aspects of the program
and then working on the synthesis aspect shortly after.&lt;/p&gt;

&lt;p&gt;I am aiming to achieve a basic working command-line-only prototype within the
next several weeks, which I will flesh out further from there after assessing
its results.&lt;/p&gt;</content><author><name>Liam Wyllie</name></author><summary type="html">Thoughts on the Program’s Direction I have been thinking a lot about my aims with the program and my report, and the particular niche of musical concatenative synthesis I would like to explore. One aspect I would like to focus more strongly on, is the notion of working with real-time audio input. Thus, CATE would primarily be an experimental tool for generating variations of an audio source in real-time. Essentially, this will be a type of audio effect, where the incoming audio is transformed according to its analysed features.</summary></entry><entry><title type="html">Handling the Audio Database</title><link href="/2019/02/10/handling-audio-file-database.html" rel="alternate" type="text/html" title="Handling the Audio Database" /><published>2019-02-10T00:00:00+00:00</published><updated>2019-02-10T00:00:00+00:00</updated><id>/2019/02/10/handling-audio-file-database</id><content type="html" xml:base="/2019/02/10/handling-audio-file-database.html">&lt;h3 id=&quot;loading-a-directory-of-audio-files&quot;&gt;Loading a Directory of Audio Files&lt;/h3&gt;
&lt;p&gt;My first objective was to find a way of automatically loading every
audio file from a directory into memory in the program.&lt;/p&gt;

&lt;p&gt;After doing some research, I realised that this functionality only
exists natively in C++ with the C++17 standard, and I am using the
earlier C++14 standard in my project for compatibility
purposes. Therefore, I decided to use the filesystem header from the
boost library. This required installing the boost headers on my system
as I have not used them before when working with C++, and modifying my
CMakeLists file. After experimenting with the boost/filesystem
functions, I eventually implemented a way of getting the path of every
file deeper than a specified directory, using a recursive process:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A first, non-recursive function generates a list of all paths from a root directory.&lt;/li&gt;
  &lt;li&gt;The recursive function calls this function as its first statement.&lt;/li&gt;
  &lt;li&gt;It then iterates over that list.&lt;/li&gt;
  &lt;li&gt;If it encounters a directory path, the recursive function is called
again, passing the path as a parameter.&lt;/li&gt;
  &lt;li&gt;Otherwise, the recursive function adds the path to the final path list.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This has been working well for the particular directory of example
sounds I am using. One particular issue that might arise is if the
user has a directory filled with a mixture of audio files and
non-audio files, which could result in the contents of those non-audio
files being treated as sample data. I may have to test for this case
and implement a way of checking if a file is an audio file or not.&lt;/p&gt;

&lt;h3 id=&quot;pre-processing-the-audio-database&quot;&gt;Pre-processing the Audio Database&lt;/h3&gt;
&lt;p&gt;My first idea was to convert every audio file loaded to the same
sample rate (the standard 44.1kHz CD-quality rate). I chose to do
this, as I had read in some papers relating to doing audio analysis on
a database of sounds that this would result in more effective
results. It also made it easier to test audio files with my program,
as it means I don’t have to do any interpolation in the actual audio
processing function. I implemented a function for converting an audio
file to a new sample rate using the
&lt;a href=&quot;https://github.com/erikd/libsamplerate&quot;&gt;libsamplerate&lt;/a&gt;
library. Although I have been generally sticking to my philosophy of
avoiding C libraries and preferring native C++ code, I was only
implementing a very small function contained within my AudioBuffer
class, so I had no problems with it in this case. I found it
preferable to use this library than implementing my own sample rate
interpolation for a number of reasons:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Likely more efficient, faster, safer etc. than an implementation of my own.&lt;/li&gt;
  &lt;li&gt;Simplifies my own codebase by requiring less code.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;That being said, I did become interested in sample rate interpolation when looking into this, and it’s something that I would like to implement myself in the future.&lt;/p&gt;

&lt;h3 id=&quot;python-utilities&quot;&gt;Python Utilities&lt;/h3&gt;
&lt;p&gt;It has been my plan with this project to make use of the Numpy and
Matplotlib libraries for the Python programming language, to help with
the creation of C++ DSP code. This week I created some simple plotting
tools which allow me to quickly view the waveform or spectrum of a
given audio file. When I am developing the audio analysis functions,
it will be helpful to prototype them in Python and plot their results
before implementing them in C++.&lt;/p&gt;

&lt;h3 id=&quot;sdif-files&quot;&gt;SDIF Files&lt;/h3&gt;
&lt;p&gt;I began to research the SDIF file format more, thinking about how I
could use it in my program. I plan to implement the simplest parts
of its functionality to begin with, reading and writing files and
printing their contents in my program. When I progress to the audio
analysis aspects later, I will start to flesh out the SDIF code more.&lt;/p&gt;

&lt;h3 id=&quot;audio-buffer-segmentation&quot;&gt;Audio Buffer Segmentation&lt;/h3&gt;
&lt;p&gt;At this point I also wanted to experiment with splitting AudioBuffer
objects into sub-objects and playing them back with a modified version
of the Synth class. This was successful, and I was able to play back
short snippets of files from the database. It made me think about how
I will handle the segmentation process later on, as I realised that
pre-segmenting every audio file will likely be impractical and
inflexible. I looked back to how existing concatenative synthesis
implementations such as CataRT deal with this, and realised that
storing segmentation positions as numeric data may be the best
option. I can then do the actual segmentation in real-time in the
Synth::process function. Right now my understanding of this process
based on my research into concatenative synthesis is as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The audio database is analysed, and segmentation markers are
generated along with descriptors.&lt;/li&gt;
  &lt;li&gt;The information is stored in files to be recalled later. Each
segment may have its own row in the database along with columns of
features extracted from the audio analysis process.&lt;/li&gt;
  &lt;li&gt;During the synthesis stage, the user specifies target parameters,
and a combination of segments from the database are generated&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Once I begin working on the the analysis and synthesis stage of the
program, I will be able to experiment with the approach more. I plan
to first implement a non-realtime version which will allow me to
experiment with different target parameters through the command line
and create automated tests, which should make the process easier,
although I will also need a way of testing its real-time capabilities.&lt;/p&gt;

&lt;h3 id=&quot;moving-forwards&quot;&gt;Moving Forwards&lt;/h3&gt;
&lt;p&gt;At this point I have already partially completed some goals from weeks
3 and 4 of the production schedule. My goal at this point is to take
some time to improve the code that I already written along with the
rest of those production targets, while doing more reading and
planning of the analysis and synthesis stage.&lt;/p&gt;

&lt;p&gt;I have still not implemented audio input recording, which I will
definitely aim to achieve in the next two weeks. I also want to
implement a better command line interface with best practises for C++
CLI programs, expanding upon the simple interface I currently have,
which directly uses the argc and argv parameters inherited from C.&lt;/p&gt;</content><author><name>Liam Wyllie</name></author><summary type="html">Loading a Directory of Audio Files My first objective was to find a way of automatically loading every audio file from a directory into memory in the program.</summary></entry><entry><title type="html">Audio Programming in C++</title><link href="/2019/02/03/audio-programming-in-c++.html" rel="alternate" type="text/html" title="Audio Programming in C++" /><published>2019-02-03T00:00:00+00:00</published><updated>2019-02-03T00:00:00+00:00</updated><id>/2019/02/03/audio-programming-in-c++</id><content type="html" xml:base="/2019/02/03/audio-programming-in-c++.html">&lt;h3 id=&quot;starting-out&quot;&gt;Starting Out&lt;/h3&gt;

&lt;p&gt;My main goal when starting the project was to first establish which
libraries for working with audio in C++ would form the basis of the
program, and how I would design and structure the audio backend of the
program. I have found that when creating audio applications in C++,
choosing between C and C++ libraries is an important choice. Usually,
the option is to either use established, widely-used C libraries like
portaudio and libsndfile, and hide their C functions behind a C++
interface, or use native C++ libraries. I have noticed that certain
applications like SuperCollider opt for the former, however wrapping C
code in C++ can be time consuming and error-prone and should generally
be avoided when possible, as I have discussed with my supervisor. In
general, I think that it’s better to use the STL and modern C++
features, rather than using the backward-compatible C parts of
C++. This post recaps my experiences with exploring both options,
comparing them and expanding upon my motivations for ultimately
deciding upon using the native C++ bindings for the portaudio and
libsndfile libraries.&lt;/p&gt;

&lt;h3 id=&quot;reading-audio-file-data&quot;&gt;Reading Audio File Data&lt;/h3&gt;
&lt;p&gt;My first objective was dealing with uncompressed audio file formats
like WAV and AIFF; Implementing functionality for reading existing
files and storing their sample values in data structures, as well as
creating output files from sequences of samples.&lt;/p&gt;

&lt;p&gt;I initially used the C library libsndfile, which provides an easy way
of dealing with audio files without having to worry about issues like
file headers and endianness. This involved creating a wrapper class
that hid the C functions behind a C++ interface.&lt;/p&gt;

&lt;p&gt;One immediate issue I found was that due to C’s lack of templates and
function overloading, each return type for the sf_read functions has a
separate function. Working around this to create generic functions
would add unnecessary complexity, if I wanted my functions to work
with both floats and doubles for instance.&lt;/p&gt;

&lt;p&gt;Therefore, I eventually decided to use the C++ API instead, which
provides a SndfileHandle class with overloaded member functions,
fitting better into a C++ program.&lt;/p&gt;

&lt;h3 id=&quot;audio-io-api&quot;&gt;Audio I/O API&lt;/h3&gt;

&lt;p&gt;Prior to starting the project, I decided upon using the C PortAudio
library as I had experience using it in the past. I achieved good
results initially, using it in a procedural manner to play back audio
file data. When I wanted to adapt the code to a more templated
object-oriented C++ style that I intended the rest of my program to be
written in, I encountered some challenges that made me reassess my
options going forwards. Wrapping the functionality in C++ classes was
proving to be time consuming and requiring a lot of workarounds, so I
decided to use the C++ bindings for the library rather than create my
own wrappers. As well as being less time-consuming and allowing me to
focus more on writing the application itself, this has the advantage
of being better tested and more reliable due to being created by the
developers of the library.&lt;/p&gt;

&lt;h3 id=&quot;progress-summary&quot;&gt;Progress Summary&lt;/h3&gt;

&lt;p&gt;So far, the program is able to load audio file data into an
AudioBuffer class, and output them through the Synth::process member
function of the Synth class. Later, these components will be expanded
upon, so that the AudioBuffers can be split into smaller segmented
buffers that can be played back simultaneously.&lt;/p&gt;

&lt;p&gt;The program also has some basic multithreading capabilities so far
using the std::thread class from C++11, with the audio stream starting
on a separate thread while the main thread continues (as a while
(true) loop for now as user input isn’t needed yet).&lt;/p&gt;

&lt;p&gt;My plans moving forward are to add functionality for storing multiple
AudioBuffers in memory from a directory of audio files, in preparation
for the audio analysis part of my program. I may use the Boost library
for a portable way of interacting with the filesystem to achieve
this. I will also look ahead to how I can store the analysis data in
some kind of lightweight database, which may involve utilising the SDIF file format.&lt;/p&gt;</content><author><name>Liam Wyllie</name></author><summary type="html">Starting Out</summary></entry></feed>