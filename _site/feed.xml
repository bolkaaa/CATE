<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2019-05-09T08:08:34+01:00</updated><id>/feed.xml</id><title type="html">CATE</title><subtitle>Project blog for the CATE program, updated with my progress and findings.</subtitle><author><name>Liam Wyllie</name></author><entry><title type="html">Overall Improvements to Program</title><link href="/2019/04/28/overall-improvements-to-program.html" rel="alternate" type="text/html" title="Overall Improvements to Program" /><published>2019-04-28T00:00:00+01:00</published><updated>2019-04-28T00:00:00+01:00</updated><id>/2019/04/28/overall-improvements-to-program</id><content type="html" xml:base="/2019/04/28/overall-improvements-to-program.html">&lt;p&gt;Compared to last week, there were many improvements to the program this week, to
its features as well as the quality of the code through refactoring. While I am
trying to avoid adding significant new features, I am highly focused on
improving the existing ones. I will break down the improvements to the program I
have made this week.&lt;/p&gt;

&lt;h3 id=&quot;synthesis&quot;&gt;Synthesis&lt;/h3&gt;
&lt;p&gt;The granulation engine is now much more flexible, as the grain size can be
controlled in real time through a slider. Overall, I am quite happy with how it
sounds and find it quite aesthetically interesting compared to previous results
I had.&lt;/p&gt;

&lt;h3 id=&quot;interface&quot;&gt;Interface&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://files.catbox.moe/1gctlq.png&quot; alt=&quot;Program Window&quot; /&gt;
I have improved the interface of the program a lot:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Value of parameters is now visible through number displays&lt;/li&gt;
  &lt;li&gt;The layout of the window is now better-organised&lt;/li&gt;
  &lt;li&gt;The window is now fixed in size, so the UI elements cannot be displaced through resizing&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The interface is now mostly complete, however I still need to add an option in the GUI to change the following audio settings:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;sample rate&lt;/li&gt;
  &lt;li&gt;buffer size&lt;/li&gt;
  &lt;li&gt;FFT bin size&lt;/li&gt;
  &lt;li&gt;input device&lt;/li&gt;
  &lt;li&gt;output device&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This will be in the form of a secondary window that provides control over each
of these elements through dropdown menus, similar to how control over these
elements is provided in software such as PureData or Max/MSP.&lt;/p&gt;

&lt;p&gt;Additionally, I made a minor improvement this week to the interface by altering
the signals and slots code such that the buttons in the interface that result in
dialog windows no longer stay highlighted after the action has been carried out.
This was solved by changing instances of the pressed() signal to clicked()
instances.&lt;/p&gt;

&lt;h3 id=&quot;audio-settings-class&quot;&gt;Audio Settings Class&lt;/h3&gt;
&lt;p&gt;As a step towards solving the problem stated previously, I have created a new
class: AudioSettings, containing the parameters mentioned above. It is designed
to be used as a “singleton” object, with one instantiation in the main function
and a reference to it passed to other objects in the program, so that they can
share the same settings. To this end, I will design the class so that the copy
constructor and assignment operator functions are private. This is a common C++
technique when you wish to disallow copying or assigning of an object to another
object, which is relevant in this case.&lt;/p&gt;

&lt;h3 id=&quot;audio-analysis&quot;&gt;Audio Analysis&lt;/h3&gt;
&lt;p&gt;Although I do not wish to add many more elements to the program, before
finalising things, I would like to improve the audio analysis code somewhat. The
spectral kurtosis feature was added recently to improve the overall unit
selection performance. I would like to spend some time if possible to improve
the feature extraction code and make it more modular. Additionally, I may add a
normalisation stage to the feature extractors.&lt;/p&gt;

&lt;h3 id=&quot;future-plans&quot;&gt;Future Plans&lt;/h3&gt;
&lt;p&gt;I am aiming to get the writing and documentation finished within the next week
or so, while also doing a bit more work and unit test writing for the program.
Then, the final week will be dedicated to improving the program, finalising the
CMake build/executables and tying up other loose ends.&lt;/p&gt;</content><author><name>Liam Wyllie</name></author><summary type="html">Compared to last week, there were many improvements to the program this week, to its features as well as the quality of the code through refactoring. While I am trying to avoid adding significant new features, I am highly focused on improving the existing ones. I will break down the improvements to the program I have made this week.</summary></entry><entry><title type="html">Many Improvements and More Testing</title><link href="/2019/04/28/many-improvements-and-more-testing.html" rel="alternate" type="text/html" title="Many Improvements and More Testing" /><published>2019-04-28T00:00:00+01:00</published><updated>2019-04-28T00:00:00+01:00</updated><id>/2019/04/28/many-improvements-and-more-testing</id><content type="html" xml:base="/2019/04/28/many-improvements-and-more-testing.html">&lt;p&gt;This was the busiest week yet of working on the program, as I wanted to improve
it as much as possible before the completion date. Due to the writing being more
or less finished by this point, I had a lot of time to do this. I tried to focus
on improving existing features rather than adding new ones. This included the
following alterations:&lt;/p&gt;

&lt;h3 id=&quot;structural-improvements&quot;&gt;Structural Improvements&lt;/h3&gt;
&lt;p&gt;In the past week, I have greatly simplified the class hierarchy of the program,
making it more readable and logical. Previously, I was instantiated the KdTree
object in main(), and passing it around the program as a reference. This was due
to my lack of understanding of an alternative way of instantiating it, which I
have now corrected. I have now moved it to the Corpus class, where it is stored
as a pointer. As its functionality is integral to the Corpus class and it can
now simply be accessed through that, I feel this is a much better way of doing
things. Likewise, I moved the PointCloud object to Corpus. I also made these
classes non-copyable, as I wanted to design the program such that only one
instance of objects created from these classes should exist. I created a base
class, &lt;em&gt;NonCopyable&lt;/em&gt;, which other classes can inherit from. This is a class
where the assignment operator and copy constructor are deleted, meaning copies
cannot be created as those functions are normally used for.&lt;/p&gt;

&lt;h3 id=&quot;multithreading&quot;&gt;Multithreading&lt;/h3&gt;
&lt;p&gt;There were several parts of the program that I was unhappy with, in terms of
efficiency. My main concern was that all operations were taking place
sequentially on the same thread. This placed limitations on how the analysis and
synthesis code could interact. It also meant that my naive recording solution
was very memory-inefficient and unacceptable as a long term solution. Due to
this, I looked into the possiblity of moving these pieces of functionality to
dedicated threads, and having them operate independently of the synthesis code.
To achieve this, I used the QThread class from the Qt library, which worked well
with the rest of the Qt code in the program. I brought back the RingBuffer class
to the project, and used it extensively as a container for the data being passed
between the threads. Its usefulness for this comes from the fact that by design
it has protection against the possibility of operations from two threads
modifying the same access point at once.&lt;/p&gt;

&lt;p&gt;Within the audio callback function, input (microphone) data is pushed into one
ring buffer, and otuput (synthesized) data is pushed into another. These are
emitted as a signal to the &lt;em&gt;AnalysisWorker&lt;/em&gt; and &lt;em&gt;RecordWorker&lt;/em&gt; objects
respectively, each of which occupy separate threads. In AnalysisWorker, the
spectral features are extracted and a KNN search finds the k-nearest data points
in the point cloud. In RecordWorker, the output data is pushed to a linked list,
which has the benefit for this use case of faster insertion time compared to a
dynamic array. The recording functionality is now working quite well, though it
still seems to drop samples at times, so it may need to be optimised further.&lt;/p&gt;

&lt;h3 id=&quot;reworked-feature-extraction-system&quot;&gt;Reworked Feature Extraction System&lt;/h3&gt;
&lt;p&gt;I wanted to create a more flexible feature extraction system that wasn’t as
dependent upon manually referring to the defined functions. The &lt;em&gt;FeatureVector&lt;/em&gt;
class contains the following members:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;A vector of FeatureExtractors, which is a simple object comprised of a string to identify the extractor, and a function pointer to the corresponding function.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An unordered/hash map of &amp;lt;string, vector&lt;float&gt;&amp;gt; pairs. This allows the lookup of a vector of extracted values via the name of the feature.&lt;/float&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A vector of markers (file positions)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Given a magnitude spectrum as input, the class can compute each feature value
and store it in the map. This class is now used within &lt;em&gt;Corpus&lt;/em&gt; as part of the
sliding window analysis process.&lt;/p&gt;

&lt;h3 id=&quot;improving-feature-extraction-functions&quot;&gt;Improving Feature Extraction Functions&lt;/h3&gt;
&lt;p&gt;As I only had two features implemented (spectral centroid and spectral
flatness), and I wasn’t happy with their implementations, I improved upon them.
Particularly for the spectral flatness function, I separate the calculations
into different functions, for calculating arithmetic and geometric mean. This
made the actual function much easier to break down and understand.&lt;/p&gt;

&lt;p&gt;I also added a new feature, spectral rolloff. This is defined as the point of
the spectrum at which a particular percentage of the overall spectrum’s energy,
often defined as 85%, is located within this point and below.&lt;/p&gt;

&lt;h3 id=&quot;large-improvements-to-synthesis-code&quot;&gt;Large Improvements to Synthesis Code&lt;/h3&gt;
&lt;p&gt;After struggling with the synthesis implementation earlier in the program’s
development and being unsatisfied with its sonic results, I had to move on to
other parts of the program and accept how it turned out. Now that I have time to
improve things, I have come back to it and reworked it entirely. The main change
is that the grains for the current audio file corpus are now computed when it’s
loaded. This means that I could remove the inefficient buffer-filling loops that
were slowing down the execution. Due to the fact that the audio files had to be
loaded into memory at all times with the previous method regardless, there is no
disadvantage to doing this. One issue is that I now have to find a strategy for
determining which grains to play back. As the actual number of available grains
created from the corpus will be very high, there needs to be a way of cycling
through a fixed set of them, as well as being able to update that set. Improving
this system will be my main priority from this point.&lt;/p&gt;

&lt;h3 id=&quot;moving-towards-a-better-input-analysis--synthesis-system&quot;&gt;Moving Towards a Better Input Analysis + Synthesis System&lt;/h3&gt;
&lt;p&gt;Moving on from the last point, I had the plan to improve how grains in the
corpus would be selected, based upon the results of the KNN search. My plan is
to have a hash table (std::unordered_map in C++) comprised of a particular
key/value pair. The key will be a integer/string pair where the integer is a
file position and the string is a file path. The value will be an integer that
corresponds to the index of the aforementioned grain container. In ideal
conditions, this will allow for an O(1) lookup of the corresponding corpus
grain, given positional file information. My priority now is to focus on taking
the marker/path pairs generated by the AnalysisWorker thread, and transporting
them to the Granulator object to calculate new indices in an effective manner.&lt;/p&gt;

&lt;h3 id=&quot;gui-improvements&quot;&gt;GUI Improvements&lt;/h3&gt;
&lt;p&gt;The sliders in the GUI now initialise to the same values as the actual
parameters, which was a much-needed improvement. Also, due to reworking the
synthesis code, the changes in audio from manipulating the sliders is now
smoother as well. I plan to improve the GUI a bit more before the completion
date, as I want to display some more information now that I have more threads to
work with.&lt;/p&gt;

&lt;h3 id=&quot;the-unit-tests&quot;&gt;The Unit Tests&lt;/h3&gt;
&lt;p&gt;The unit testing has also been well underway in the past week. It has made me
regret not writing them sooner and taking a more test-driven development focused
approach, as I have already improved my code a lot due to issues discovered from
the tests I’ve written. My tests have been separated into the following categories, so far:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Audio&lt;/em&gt;: The PortAudio backend and ensuring it starts and stops properly.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Analysis&lt;/em&gt;: The creation of magnitude spectrum arrays
from audio file segments. It will also test the various feature extractors used in the program.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Corpus&lt;/em&gt;: The operations contained within the Corpus class, such as reading and writing the JSON files correctly.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;FileInputOutput&lt;/em&gt;: The reading and writing of audio files.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Synthesis&lt;/em&gt;: The synthesis processes used in the program.&lt;/p&gt;

&lt;p&gt;I plan to write a lot more tests over the next week as I finalise the program
and make sure that it’s as good and bug-free as it can be.&lt;/p&gt;</content><author><name>Liam Wyllie</name></author><summary type="html">This was the busiest week yet of working on the program, as I wanted to improve it as much as possible before the completion date. Due to the writing being more or less finished by this point, I had a lot of time to do this. I tried to focus on improving existing features rather than adding new ones. This included the following alterations:</summary></entry><entry><title type="html">Lots of Writing and Not Much Progress</title><link href="/2019/04/21/lots-of-writing-and-not-much-progress.html" rel="alternate" type="text/html" title="Lots of Writing and Not Much Progress" /><published>2019-04-21T00:00:00+01:00</published><updated>2019-04-21T00:00:00+01:00</updated><id>/2019/04/21/lots-of-writing-and-not-much-progress</id><content type="html" xml:base="/2019/04/21/lots-of-writing-and-not-much-progress.html">&lt;p&gt;This week there were less changes made in the program as development has started slowing down due to the program mostly being complete, and the fact that I need to work on the project writeup. Most of the changes were minor code cleanups and some interface improvements. Notably, I also improved aspects of the program’s design through separating some functionality into dedicated classes, in the case of AudioRecorder. Next week’s post will be much lengthier as I return to working on the program more intensely, as the writing gets closer to the completion and I try to make the program as good as it can be.&lt;/p&gt;</content><author><name>Liam Wyllie</name></author><summary type="html">This week there were less changes made in the program as development has started slowing down due to the program mostly being complete, and the fact that I need to work on the project writeup. Most of the changes were minor code cleanups and some interface improvements. Notably, I also improved aspects of the program’s design through separating some functionality into dedicated classes, in the case of AudioRecorder. Next week’s post will be much lengthier as I return to working on the program more intensely, as the writing gets closer to the completion and I try to make the program as good as it can be.</summary></entry><entry><title type="html">Finalising Some Features and Starting Unit Testing</title><link href="/2019/04/14/finalising-some-features-and-starting-unit-testing.html" rel="alternate" type="text/html" title="Finalising Some Features and Starting Unit Testing" /><published>2019-04-14T00:00:00+01:00</published><updated>2019-04-14T00:00:00+01:00</updated><id>/2019/04/14/finalising-some-features-and-starting-unit-testing</id><content type="html" xml:base="/2019/04/14/finalising-some-features-and-starting-unit-testing.html">&lt;h3 id=&quot;progress&quot;&gt;Progress&lt;/h3&gt;
&lt;p&gt;While continuing to work on the report, I have also added some final features in
the software. These included adding amplitude following of the microphone input
for the synthesis output, and adding the functionality for recording output to a
file.&lt;/p&gt;

&lt;h3 id=&quot;amplitude-following&quot;&gt;Amplitude Following&lt;/h3&gt;
&lt;p&gt;One feature that I have intended to be in the program for a while but only
recently implemented was a relationship between the audio input and output.
Essentially, I wanted the amplitude of the synthesised output to be the same as
the amplitude of the microphone input. Without such a feature, the resulting
effect is that quiet and intense input gestures would result in the same level
of output, which I wanted to avoid. To achieve this, I implemented a calculation
of the &lt;em&gt;root mean square&lt;/em&gt; (RMS) value of the input samples:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sqrt{\frac{1}{n}\sum_{i=0}^{n}x_i^{2}}&lt;/script&gt;

&lt;p&gt;This value is multiplied with the output sample for each sample in the callback
function, which has the effect of the output amplitude closely following the
input amplitude.&lt;/p&gt;

&lt;h3 id=&quot;recording-to-file&quot;&gt;Recording to File&lt;/h3&gt;
&lt;p&gt;One drawback of creating a standalone audio synthesis/processing application as
opposed to a DAW/host-compatible plugin is that there is no immediate way of
having an output for the audio that can be used elsewhere. With a plugin, the
audio can go directly to the DAW and be used there. With my program, I had to
implement a way of having the audio output from the program be usable in other
contexts. My way of achieving this was to implement audio recording, so that the
user can experiment however they like, and save the resulting output to an audio
file. This approach is similar to how applications such as IRCAM’s AudioSculpt
are designed to be used.&lt;/p&gt;

&lt;p&gt;I delayed implementing this feature for some time as I was unsure of how to
achieve it in an efficient manner. Eventually, I realised it was more important
to have it working, even if the implementation was not ideal. The basic
algorithm I designed was as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A high capacity, fixed-size array is instantiated along with the other objects in the AudioProcess class.&lt;/li&gt;
  &lt;li&gt;A boolean parameter determines whether the program is in “record mode” or not.&lt;/li&gt;
  &lt;li&gt;In the GUI, a button is provided that sets the parameter to &lt;em&gt;true&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;In the main audio loop, the output samples are written to the array according to the condition of the recording parameter being equal to &lt;em&gt;true&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;This simply uses a counter that increments on each successive sample to index
into the array, rather than some kind of “push back” container function,
which would be too inefficient in an audio context.&lt;/li&gt;
  &lt;li&gt;When the user is done and has pressed the “Stop Recording” button, the size
of the resultant file in samples is determined by the value of the counter
after recording has stopped. A WAV/AIFF file is written to disk using the
sample data from the array.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The main drawback I found with this approach is that it’s quite
memory-inefficient, since a large array needs to be preallocated. I couldn’t
find a better way of doing it however, as I figured that any approaches that
would be both space-efficient and time-efficient would likely require some kind
of buffering system with multiple threads, which I did not want to implement
with the constraints of how I had previously written the program.&lt;/p&gt;

&lt;h3 id=&quot;unit-testing&quot;&gt;Unit Testing&lt;/h3&gt;
&lt;p&gt;While I have been busy adding the previously mentioned features and writing my
dissertation, I have also been spending more time reading documentation for the
Catch2 framework. In the next week I aim to get the functionality for creating
tests working with my build system. The first tests I will write will likely be
for testing the FFT and feature extraction code.&lt;/p&gt;

&lt;h3 id=&quot;future-plans&quot;&gt;Future Plans&lt;/h3&gt;
&lt;p&gt;Currently, there is approximately one month left to finish working on the
program and writing the dissertation. Ideally, as discussed with my supervisor,
I should try to have a feature freeze and stop working on new features, and
focus on bug fixing, error handling and unit testing. Currently, the only
features I really intend to work on are improving the analysis code and the GUI.
In the next week, I hope to have these finished while starting on unit tests.&lt;/p&gt;</content><author><name>Liam Wyllie</name></author><summary type="html">Progress While continuing to work on the report, I have also added some final features in the software. These included adding amplitude following of the microphone input for the synthesis output, and adding the functionality for recording output to a file.</summary></entry><entry><title type="html">More GUI Work</title><link href="/2019/04/07/more-GUI-work.html" rel="alternate" type="text/html" title="More GUI Work" /><published>2019-04-07T00:00:00+01:00</published><updated>2019-04-07T00:00:00+01:00</updated><id>/2019/04/07/more-GUI-work</id><content type="html" xml:base="/2019/04/07/more-GUI-work.html">&lt;h3 id=&quot;progress&quot;&gt;Progress&lt;/h3&gt;
&lt;p&gt;There has been less progress with the software this week as I have been trying to do more writing. One significant 
change I have added is that the user may now select a directory of audio files to use when running the program. They 
may also choose a JSON file of analysis data that they have already saved in an previous session.&lt;/p&gt;

&lt;h3 id=&quot;gui-improvements&quot;&gt;GUI Improvements&lt;/h3&gt;
&lt;p&gt;To elaborate upon this, the program now has buttons that open dialog prompts for selecting directories of audio 
files and analysing them, creating a more user-friendly experience than the command line system I had previously 
implemented. This required learning more about Qt and its QFileDialog class.&lt;/p&gt;

&lt;h3 id=&quot;testing&quot;&gt;Testing&lt;/h3&gt;
&lt;p&gt;I plan to start implementing unit tests soon. After doing some research, I found that 
&lt;a href=&quot;https://github.com/catchorg/Catch2&quot;&gt;Catch2&lt;/a&gt; seems to be a good option. It can be included in a C++ project as a 
single header file, and œmakes use of modern, idiomatic C++. Along with unit tests, I also plan to start testing the 
audio functionality of my program with manual tests of different types of audio. A significant section of my report 
will be a study of how the program responds to different types of audio, whether the effects are desirable or less so.&lt;/p&gt;

&lt;h3 id=&quot;finalising-features&quot;&gt;Finalising Features&lt;/h3&gt;
&lt;p&gt;It will be helpful to finish adding features to the program in the next week, so that I can focus on testing and 
writing my report. Fortunately, I have kept to my schedule well, and I have around a month to not only work on the 
report, but tie up loose ends with the program as well as do unit testing. The features I would like to finish with are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;More feature extraction functions&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I will likely just add more spectral functions, since I already have the FFT code in place to easily implement them. 
So far, I am quite happy with how the current features relate the audio input to the synthesis output, but having 
more features would give more control over this process.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Synthesis improvements&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Implementing interpolation of sample values to have control over grain pitch would be a useful feature, but I have had 
trouble implementing this in the past.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Amplitude following of microphone input to control the output&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Currently, the amplitude of the output is static regardless of the current microphone input. I intend to have them be
 correlated, so that quiet input sounds result in a quieter output and vice versa.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;More controls in the GUI
The main controls I plan to add are for configuring the user’s inputs and outputs. Right now, it just uses the system
 defaults. For specific audio setups such as those that make use of external audio interfaces, this may not be 
 desirable.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once I’ve finished with adding features to the program, I can focus on improving error handling and testing. I can 
also do some refactoring to improve the code and make it more readable and efficient while keeping the functionality 
the same.&lt;/p&gt;</content><author><name>Liam Wyllie</name></author><summary type="html">Progress There has been less progress with the software this week as I have been trying to do more writing. One significant change I have added is that the user may now select a directory of audio files to use when running the program. They may also choose a JSON file of analysis data that they have already saved in an previous session.</summary></entry><entry><title type="html">Structural Changes and Working on GUI</title><link href="/2019/03/31/structural-changes-and-working-on-GUI.html" rel="alternate" type="text/html" title="Structural Changes and Working on GUI" /><published>2019-03-31T00:00:00+00:00</published><updated>2019-03-31T00:00:00+00:00</updated><id>/2019/03/31/structural-changes-and-working-on-GUI</id><content type="html" xml:base="/2019/03/31/structural-changes-and-working-on-GUI.html">&lt;h3 id=&quot;progress&quot;&gt;Progress&lt;/h3&gt;
&lt;p&gt;I have focused on writing my report this week, but have also made some
improvements and additions to the program, which I will discuss in the following sections.&lt;/p&gt;

&lt;h3 id=&quot;structural-changes&quot;&gt;Structural Changes&lt;/h3&gt;
&lt;p&gt;Previously, my program was designed such that the AudioProcess object that
contains most of the audio functionality of the program was a member of the
MainWindow class. This caused a coupling of backend and frontend logic that I
wanted to move away from. Therefore I am now instantiating the AudioProcess
object earlier in the main function and passing it by reference to MainWindow,
which is a more flexible way of working.&lt;/p&gt;

&lt;h3 id=&quot;more-granular-synthesis-improvements&quot;&gt;More Granular Synthesis Improvements&lt;/h3&gt;
&lt;p&gt;The grain envelope implementation now functions much more smoothly, with control
over attack and release of each grain, using an ASR envelope shape. I first
prototyped this in Python which was very helpful due to being able to plot the
tests with Matplotlib. I also implementing functionality for modifying the
envelope parameters, which I will come back to in a later section of this post.
Another improvement was in the grain inter-onset generator. An average onset
time in samples is calculated by dividing the audio system’s sample rate by a
&lt;em&gt;grain density&lt;/em&gt; parameter, which is effectively the average number of grains per
second. A &lt;em&gt;grain width&lt;/em&gt; parameter determines how far apart a minimum and maximum
onset are from the average onset. Then, the random inter-onset time is
calculated based upon these values.&lt;/p&gt;

&lt;p&gt;The overall sound is still not perfect, but has improved a lot in the past few
weeks. I will now be aiming to focus on improving other parts of the program
rather than the sound of the granulation in particular.&lt;/p&gt;

&lt;h3 id=&quot;starting-to-work-more-on-the-qt-gui&quot;&gt;Starting To Work More On The Qt GUI&lt;/h3&gt;
&lt;p&gt;This week I also started doing more work on the Qt GUI of the program. I am
using &lt;em&gt;Qt Designer&lt;/em&gt; for a WYSIWYG (What You See Is What You Get) approach to GUI
design. At the moment, the interface has buttons for starting and stopping audio
playback, along with sliders for controlling certain synthesis parameters. As
mentioned earlier, I had to create a new class for storing parameters for the
grain envelopes, which is passed around in the granular synthesis object system.
This allows for control of the grain attack and release from the top-level
object in the hierarchy. The list of controllable parameters so far is as
follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Overall Amplitude&lt;/li&gt;
  &lt;li&gt;Grain Attack&lt;/li&gt;
  &lt;li&gt;Grain Release&lt;/li&gt;
  &lt;li&gt;Grain Density&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I will likely add more parameters later, along with other potential additions
such as analysis visualisations.&lt;/p&gt;

&lt;h3 id=&quot;next-goals&quot;&gt;Next Goals&lt;/h3&gt;
&lt;p&gt;I had meant to include functionality for recording audio output in one of my
earlier weekly schedule targets, but I never got around to doing so. One
disadvantage of creating a standalone audio processing application as opposed to
a VST/AU plugin, is that this kind of functionality must be implemented, which
can be difficult. For the program to be truly &lt;em&gt;useful&lt;/em&gt;, it should have this kind
of functionality. Otherwise, it is more of a program solely for experimentation
rather than musical usefulness. Therefore I will try to get this functionality
working as soon as possible. Besides that, I will be doing more work on the GUI
as well as thinking about including unit tests to systematically test the
correctness of certain aspects of the program.&lt;/p&gt;</content><author><name>Liam Wyllie</name></author><summary type="html">Progress I have focused on writing my report this week, but have also made some improvements and additions to the program, which I will discuss in the following sections.</summary></entry><entry><title type="html">Expanding Upon Granular Synthesis</title><link href="/2019/03/24/expanding-upon-granular-synthesis.html" rel="alternate" type="text/html" title="Expanding Upon Granular Synthesis" /><published>2019-03-24T00:00:00+00:00</published><updated>2019-03-24T00:00:00+00:00</updated><id>/2019/03/24/expanding-upon-granular-synthesis</id><content type="html" xml:base="/2019/03/24/expanding-upon-granular-synthesis.html">&lt;h3 id=&quot;progress&quot;&gt;Progress&lt;/h3&gt;
&lt;p&gt;This week I expanded upon the granular synthesis code I had created in the
previous week. The grains are now properly derived from the audio files matched
from the analysis process. The synthesis process is also more efficient and
correctly implemented, using object pooling techniques. I also started working
on a command line interface for managing the analysis process.&lt;/p&gt;

&lt;h3 id=&quot;command-line-utility&quot;&gt;Command Line Utility&lt;/h3&gt;
&lt;p&gt;One aspect of the program I wanted to develop was a basic command line utility
to generate new analysis data from specified directories of audio files. I used
a C++ library called “args” to handle the implementation details, allowing my
program to have a sophisticated CLI tool with argument flags. So far, it can be
used in the following way:&lt;/p&gt;

&lt;p&gt;CATE_CLI -f 1024 -b 1024 analyse /directory/&lt;/p&gt;

&lt;p&gt;where -f and -b specify the frames per buffer and FFT bin size parameters of the
analysis system respectively. I included pre-processor directives in my C++ code
and tweaked my build system so that multiple binaries are built from the source
code, the CLI tool and the GUI application. I will probably also have a separate
application that just executes the audio callback without a GUI.&lt;/p&gt;

&lt;h3 id=&quot;granular-synthesis-improvements&quot;&gt;Granular Synthesis Improvements&lt;/h3&gt;
&lt;p&gt;I fixed some issues I had with my granular synthesis model, relating to grain
activation/deactivation. I achieved this by incorporating a sample counter
within the synthesis function of the Grain class, so that the objects are able
to keep track of when they should be inactive. Essentially, the synthesize
function within the Grain class first checks if there are any more samples to
generate, and returns a zero-amplitude sample if the resultant boolean value is
false. Otherwise, it synthesizes a new sample and decrements the sample counter.&lt;/p&gt;

&lt;p&gt;The grain creation function within the scheduler now iterates over the array of
grain objects until it finds an inactive grains, using that same is_active()
function. It then creates a new Grain object and immediately returns from the
function. This function is called on every interonset trigger.&lt;/p&gt;

&lt;p&gt;The program now sounds reminiscent of classic granular synthesis. One issue is
there is a lack of control over grain duration, which limits the possibilities
in the sound. This is due to the fact that the grain duration is implemented as
a constant, tied to the segmentation sizes of the audio analysis system. I have
decided to just have a max duration that’s tied to the maximum buffer size, and
then have the envelope duration be controllable.&lt;/p&gt;

&lt;h3 id=&quot;relating-the-synthesis-and-analysis-processes&quot;&gt;Relating the Synthesis and Analysis Processes&lt;/h3&gt;
&lt;p&gt;At the moment, the KNN function is called on each frame within the “frames per
buffer” loop in the audio callback function. It finds just one nearest neighbour
which is immediately used by the granular synthesis code. One approach may have
been to run this code on a separate thread, and have a queue of neighbours that
is consumed by the audio process. In the next week I will finalise this part of
the program, which will explore these options.&lt;/p&gt;

&lt;p&gt;The audio files in the database are provided to the Scheduler in a C++ std::map
structure, which is keyed by a string filename. The filename is obtained from
the KNN function along with a segmentation marker position, which allows
creating a new audio buffer at the given position in the respective audio file.
The duration of the buffer must be fixed, however control over duration could be
allowed through manipulating the amplitude envelope of grains.&lt;/p&gt;

&lt;h3 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h3&gt;
&lt;p&gt;For the next week, I aim to improve the analysis and synthesis process in
general, adding more audio features and making the synthesis process more
aesthetically pleasing, such as envelope following of the microphone input’s
amplitude to control the audio output’s amplitude. I am hoping to finish most of
the functionality of the program in the next week or two so that I can focus on
the interface, error handling and unit tests.&lt;/p&gt;</content><author><name>Liam Wyllie</name></author><summary type="html">Progress This week I expanded upon the granular synthesis code I had created in the previous week. The grains are now properly derived from the audio files matched from the analysis process. The synthesis process is also more efficient and correctly implemented, using object pooling techniques. I also started working on a command line interface for managing the analysis process.</summary></entry><entry><title type="html">Implementing Simple Granular Synthesis</title><link href="/2019/03/17/implementing-simple-granular-synthesis.html" rel="alternate" type="text/html" title="Implementing Simple Granular Synthesis" /><published>2019-03-17T00:00:00+00:00</published><updated>2019-03-17T00:00:00+00:00</updated><id>/2019/03/17/implementing-simple-granular-synthesis</id><content type="html" xml:base="/2019/03/17/implementing-simple-granular-synthesis.html">&lt;h3 id=&quot;progress&quot;&gt;Progress&lt;/h3&gt;
&lt;p&gt;My goal for this week and the following week was working on a granular synthesis implementation. For the first week, I
 have aimed to just get a simple system working that has the core functionality needed going forwards. Then I plan to
  integrate it with my audio analysis code, so that the grains are derived from the matches found by the KNN search.&lt;/p&gt;

&lt;h3 id=&quot;establishing-the-granular-synthesis-model&quot;&gt;Establishing the Granular Synthesis Model&lt;/h3&gt;
&lt;p&gt;I wanted to find a system for implementing granular synthesis in C++ that would have robust foundations and be 
modular/expandable. 
After doing a 
bit of research, I found the following &lt;a href=&quot;http://www.rossbencina.com/static/code/granular-synthesis/BencinaAudioAnecdotes310801.pdf&quot;&gt;article&lt;/a&gt; written by Ross Bencina, who also designed the PortAudio library I am 
using in my program. It seemed to be an ideal starting point, so I set out to understand it and implement the class 
structure proposed within it. In the article, a model for granular synthesis is broken down into different objects, 
each of which I will write briefly about:&lt;/p&gt;

&lt;h4 id=&quot;source&quot;&gt;Source&lt;/h4&gt;
&lt;p&gt;The audio source used in the granulation, which can be of the following types:&lt;/p&gt;
&lt;h5 id=&quot;tapped-delay-line&quot;&gt;Tapped Delay Line&lt;/h5&gt;
&lt;p&gt;Sample data is stored from a real-time stream as a delay line. This is
particularly useful for granulators that operate as an audio effect. It can be
seen as the most complicated type of granular synthesis to implement, because
the delay line must be maintained and variable rate delay taps have to be
implemented for each grain.&lt;/p&gt;

&lt;h5 id=&quot;synthetic-grains&quot;&gt;Synthetic Grains&lt;/h5&gt;
&lt;p&gt;Each grain is synthesized through synthesis techniques such
as FM synthesis. Potentially simple to implement, because state doesn’t need to
be shared between grains.&lt;/p&gt;

&lt;h5 id=&quot;stored-samples&quot;&gt;Stored Samples&lt;/h5&gt;
&lt;p&gt;Sample values for grains are stored in wavetables, which can be used to process
pre-generated waveforms or sample data from audio files.&lt;/p&gt;

&lt;p&gt;As the goal of CATE is to resynthesize the audio from audio files, stored
samples will be the type of source.&lt;/p&gt;

&lt;h4 id=&quot;envelope&quot;&gt;Envelope&lt;/h4&gt;
&lt;p&gt;Generates sample values for an amplitude envelope. Types of envelope include simple parabolic envelopes, 
trapezoidal envelopes, or raised cosine bell envelopes. Grain envelopes can introduce spectral artifacts.
 Trapezoidal envelopes introduce the most spectral distortion due to discontinuities at the edges. An 
 alternative is raised cosine bell envelopes which are better in this regard, at the expense of greater processing 
 time.&lt;/p&gt;

&lt;p&gt;There is a general time/space tradeoff with implementing amplitude envelopes, where you
 may choose to either compute the coefficients on demand or use a pre-computed
 wavetable. Bencina states that modern processors allow for a more efficient
 implementation when not using stored tables, due to the memory access overhead
 of accessing stored tables.&lt;/p&gt;

&lt;p&gt;I chose to implement a Parabolic envelope for now due to its relative
 simplicity, which I can always swap out later for a raised cosine bell envelope,
 which allows for more control over the envelope parameters.&lt;/p&gt;

&lt;h4 id=&quot;grain&quot;&gt;Grain&lt;/h4&gt;
&lt;p&gt;Combines an Envelope and Source object to form an overall model of a grain in granular synthesis. The
 source sample is multiplied by the envelope sample when synthesized.&lt;/p&gt;

&lt;h4 id=&quot;scheduler&quot;&gt;Scheduler&lt;/h4&gt;
&lt;p&gt;Generates time periods between grains and handles activating them. It also manages mixing and outputting active 
grains. A simple counting system generates a new &lt;a href=&quot;https://en.wikipedia.org/wiki/Time_point#Interonset_interval&quot;&gt;interonset&lt;/a&gt; 
time whenever the sample counter is equal to 0, and activates the 
next grain. Then, that sample counter is decremented. When it hits 0 again, the cycle continues and the next grain is
 activated. The interonset time can be generated with a simple function that uses a random number generator. I used 
functionality from the random header in C++ to get uniform distribution random numbers.&lt;/p&gt;

&lt;h4 id=&quot;granulator&quot;&gt;Granulator&lt;/h4&gt;
&lt;p&gt;The top-level class that contains the scheduler and any other necessary high-level functionality for 
using the synth. This is the class that will have a spawned object in the AudioProcess class of CATE, while the 
previous classes will all be hidden.&lt;/p&gt;

&lt;h3 id=&quot;challenges-to-overcome-going-forwards&quot;&gt;Challenges to Overcome Going Forwards&lt;/h3&gt;
&lt;p&gt;After having implemented some basic granular synthesis, I feel that I have now
come to one of the more challenging aspects of implementing a granular system
that works with a large set of audio files, where the sliced segments within the files to be used as grains is 
constantly changing. Currently, I know that I have the following functionality to work with:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A granular synthesis system that requires each grain to be provided with a source wavetable array.&lt;/li&gt;
  &lt;li&gt;An audio analysis pipeline that relates lists of analysis features and segmentation positions with audio files&lt;/li&gt;
  &lt;li&gt;A real-time analysis system that outputs the same features that are used in the stored analysis data&lt;/li&gt;
  &lt;li&gt;A k-nearest neighbours function that returns the N nearest indices from the list of segmentation markers that 
correspond to the 
current block of input from the microphone&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My task now is to tie these systems together, so that the segments extracted
from the audio files are granulated according to their features that are matched
with the microphone input.&lt;/p&gt;

&lt;h3 id=&quot;current-progress-check-and-future-plans&quot;&gt;Current Progress Check and Future Plans&lt;/h3&gt;
&lt;p&gt;I am generally satisfied with my progress this week, as the most difficult aspect was approaching a topic like 
granular synthesis and understanding how to implement it in C++, while I am still new to low-level audio programming.
 I’m happy with the article written by Bencina that I found, as it improved my understanding of granular synthesis 
 and helped me get started on implementing it. I plan to spend the next week improving upon what I have, and properly
  combining it with the analysis aspects of the program. I may look into the concept of &lt;a href=&quot;https://en.wikipedia.org/wiki/Object_pool_pattern&quot;&gt;object pooling&lt;/a&gt;, which is a 
  design pattern that specifies a set of pre-initialised objects that can reused, rather than destroying objects and 
  allocating new ones. Pooled objects can be recalled in a determinant amount of time as opposed to allocated 
  objects, which may take an indeterminant amount of time (an undesirable property in audio programming).&lt;/p&gt;</content><author><name>Liam Wyllie</name></author><summary type="html">Progress My goal for this week and the following week was working on a granular synthesis implementation. For the first week, I have aimed to just get a simple system working that has the core functionality needed going forwards. Then I plan to integrate it with my audio analysis code, so that the grains are derived from the matches found by the KNN search.</summary></entry><entry><title type="html">Sliding Window Analysis and K-d Tree</title><link href="/2019/03/10/sliding-window-analysis-and-kd-tree.html" rel="alternate" type="text/html" title="Sliding Window Analysis and K-d Tree" /><published>2019-03-10T00:00:00+00:00</published><updated>2019-03-10T00:00:00+00:00</updated><id>/2019/03/10/sliding-window-analysis-and-kd-tree</id><content type="html" xml:base="/2019/03/10/sliding-window-analysis-and-kd-tree.html">&lt;h3 id=&quot;progress&quot;&gt;Progress&lt;/h3&gt;
&lt;p&gt;This week I have been building upon the audio analysis code implemented
previously. I have added functionality to perform that analysis on fixed audio
files as well as on the live input that was already present. I also added the
K-d Tree implementation to perform nearest neighbour searches, using the
&lt;a href=&quot;https://github.com/jlblancoc/nanoflann&quot;&gt;nanoflann&lt;/a&gt; library. I also fixed some
critical bugs with my program, as well as simplifying the codebase.&lt;/p&gt;

&lt;h3 id=&quot;code-simplifications&quot;&gt;Code Simplifications&lt;/h3&gt;
&lt;p&gt;Previously, I had a class design for audio data which was meant to encompass
audio data loaded from files as well as sample data in general, for example
buffers created from mathematical functions. I wasn’t happy with how I had
implemented this, where the sample data itself was encapsulated in a private
member variable, with public member functions for operations such as file
reading. The problem with this was that I had to make heavy usage of getter
functions to access the data in other classes, which defeated the purpose of it
for me. Therefore I decided to just work in a simpler way, passing std::vectors
of sample data around instead, and having specific, simple classes such as an
AudioFile class that just had the vector with public access. I used a typedef to
alias the vector to the name AudioBuffer to make it more symbolic and easier to
work with, and separate its meaning from the specific std::vector container. I
also removed a few functions from my program that I am not using currently,
which I can always add again later if necessary.&lt;/p&gt;

&lt;h3 id=&quot;fixing-a-painful-memory-bug&quot;&gt;Fixing a Painful Memory Bug&lt;/h3&gt;
&lt;p&gt;Recently I have been having some problems with my program relating to memory
that I was struggling to debug due to the nature of such issues. My program was
having issues with segmentation faults and other memory-related errors. I
eventually solved it by taking an approach where I would comment out all but the
most critical parts of my program and recompile, until the issue had gone away.
Then, I would continuously uncomment, recompile and test in a cycle, waiting to
find the combination of code that resulted in the memory errors. While going
through this process, I removed the spectrogram plotting code, as I no longer
planned to use it, and I also suspected it may be part of the problem. This
suspicion proved warranted, as my program is now free of those memory errors,
which is extremely good going forwards.&lt;/p&gt;

&lt;h3 id=&quot;sliding-window-analysis-of-audio-files&quot;&gt;Sliding Window Analysis of Audio Files&lt;/h3&gt;
&lt;p&gt;Previously, I had implemented FFT routines as well as functions for calculating
two different spectral features: spectal centroid and spectral flatness, which I
had successfully implemented in my audio callback function.&lt;/p&gt;

&lt;p&gt;My next goal was to implement this same functionality, but on frames of audio
files. The frame size should be equal to the frames per buffer variable I was
using in my audio callback function, so that the extracted data was equivalent
in meaning. I created functions that iterated over the loaded audio files,
moving a window of &lt;frame_size&gt; frames, and passing those subvectors to my FFT
and spectral feature pipeline. Then, this data is able to be stored in the
analysis files, along with the marker positions and filenames.&lt;/frame_size&gt;&lt;/p&gt;

&lt;h3 id=&quot;k-d-tree-implementation&quot;&gt;K-d Tree Implementation&lt;/h3&gt;
&lt;p&gt;A K-d tree is a data structure that partitions data in such a way that you can
perform k-nearest neighbour searches of the data much more efficiently. The
search time complexity when using a K-d tree is an average case of O(log n),
whereas regular KNN searches are at best O(nd) from the research I have done,
which is significantly slower. One tradeoff is there is some time needed to
build the tree, but this is not nearly as important as the time spent doing the
search itself, as the search will be taking place in the more critical real-time
audio part of my code.&lt;/p&gt;

&lt;p&gt;As mentioned previously, I used the nanoflann library for this. I had done some
research, and it seemed like the perfect option, as one of its main priorities
is speed and efficiency, which is important for real-time concerns of my
program. My utilisation of it required some particular details. Notably, the
library is designed in such a way that means you may only have one instance of
the K-d tree object in place in your program at a time, as its copy constructor
and assignment operators are deleted. Therefore, I implemented the object in the
main function of my program. Then, a reference to that object is passed to
functionality that needs it. Essentially, you create a primitive data structure
where your data is stored as points. You create a std::vector of these point
objects, which is used by the K-d tree object to contruct the tree index. Then,
the workflow is as simple as calling one function that takes the following
arguments:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;An array of input points&lt;/li&gt;
  &lt;li&gt;The number of search results to return&lt;/li&gt;
  &lt;li&gt;A std::vector to store the return indices of your container of data points found by the search&lt;/li&gt;
  &lt;li&gt;Another std::vector to store the actual distances of those indices from the input points&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I have added a call to it within my audio callback function, and it is operating
smoothly so far. I am passing it the centroid and flatness values generated from
the microphone input, and it is outputting the files and markers in the audio
file database that are closest in distance to the current values. My plan is to
use this information to drive a granular synthesis process, where the selected
grains are determined by the file IDs and segmentation markers.&lt;/p&gt;

&lt;h3 id=&quot;moving-onwards&quot;&gt;Moving Onwards&lt;/h3&gt;
&lt;p&gt;In my production schedule, the next two weeks were allocated towards the
granular synthesis implementation. I am happy about this, as it means that I
have been following and meeting my production schedule perfectly. I plan to take
my time and first implement a very simple synthesis system. Then I can add more
complicated features later. I’m particularly interested in how I can break down
the granular/concatenative synthesis process into different constituent parts,
in a flexible way. I will be looking to different code examples to help me
achieve the best results possible.&lt;/p&gt;</content><author><name>Liam Wyllie</name></author><summary type="html">Progress This week I have been building upon the audio analysis code implemented previously. I have added functionality to perform that analysis on fixed audio files as well as on the live input that was already present. I also added the K-d Tree implementation to perform nearest neighbour searches, using the nanoflann library. I also fixed some critical bugs with my program, as well as simplifying the codebase.</summary></entry><entry><title type="html">Analysis Functionality and Data Storage</title><link href="/2019/03/03/analysis-functionality-and-data-storage.html" rel="alternate" type="text/html" title="Analysis Functionality and Data Storage" /><published>2019-03-03T00:00:00+00:00</published><updated>2019-03-03T00:00:00+00:00</updated><id>/2019/03/03/analysis-functionality-and-data-storage</id><content type="html" xml:base="/2019/03/03/analysis-functionality-and-data-storage.html">&lt;h3 id=&quot;progress&quot;&gt;Progress&lt;/h3&gt;
&lt;p&gt;I have been implementing functionality for audio analysis and storage for the program. I am on schedule with this and will continue working on it next week as well.&lt;/p&gt;

&lt;p&gt;This week’s additions are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Visualisation of the magnitude spectrum&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Implementing spectral analysis functions&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Implementing persistent storage of analysis data in JSON format&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;General Changes to Code Design&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;visualisation-of-the-magnitude-spectrum&quot;&gt;Visualisation of the Magnitude Spectrum&lt;/h3&gt;
&lt;p&gt;I wanted a way of verifying the workings of my magnitude spectrum computation with visual feedback. I used a &lt;a href=&quot;http://www.martin-kumm.de/wiki/doku.php?id=05Misc:A_Template_for_Audio_DSP_Applications&quot;&gt;code example&lt;/a&gt; found online as a guideline when working on this. I will likely have a very different GUI by the completion of my program, but for now it has been helpful for this purpose, as well as teaching me more about Qt and the Qt plotting library. I may also add some extra plotting of my audio features, as a way of verifying their results easily as well.&lt;/p&gt;

&lt;h3 id=&quot;implementing-spectral-analysis-functions&quot;&gt;Implementing Spectral Analysis Functions&lt;/h3&gt;
&lt;p&gt;This week, I initially spent some time working with the &lt;a href=&quot;https://essentia.upf.edu/&quot;&gt;Essentia&lt;/a&gt; library, discovering if it would be suitable for my needs. In the end, I decided not to use it. All of the provided examples of its usage are non-real time utilisations, where analysis data is written to a file. It is important for my program to have real-time analysis. I found there to be very little guidance on doing this with Essentia, with the one example provided using an out of date version of Essentia, with numerous functions used within it that no longer exist in the library.&lt;/p&gt;

&lt;p&gt;Therefore, I decided to implement my own audio analysis functionality, which was made easier by the fact that I have already worked on the FFT code. Leading on from that, I chose to implement two features in particular: spectral centroid and spectral flatness. Some reasons for this are as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;They are relatively easy to calculate if you already have a magnitude spectrum computed, which I had functionality for with my previous week’s work&lt;/li&gt;
  &lt;li&gt;Compared to many other features, they are quite perceptually useful for a broad range of sounds that aren’t necessarily tonal or rhythmic, which is ideal for the audio sources my program is intended to be making use of. Centroids are linked to brightness in timbre, and flatness is a noisiness ratio where 1.0 is white noise and 0.0 is sinusoidal.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I am focusing on implementing just two features right now, and then moving on to working on the nearest neighbours search functionality. Once that is functioning correctly, I will look to add more analysis functions.&lt;/p&gt;

&lt;p&gt;Currently, the analysis code is structured in a way that there is a base “Feature” class that contains data like the sample rate of the system, which other feature classes can inherit from. I may simplify this later if the polymorphism-based approach seems unnecessary, but for now it seems like a logical way to do it.&lt;/p&gt;

&lt;h3 id=&quot;implementing-persistent-storage-of-analysis-data-in-json-format&quot;&gt;Implementing Persistent Storage of Analysis Data in JSON Format&lt;/h3&gt;
&lt;p&gt;It was important to add a way of persistently storing audio file paths and their associated analysis data arrays in some kind of data format.&lt;/p&gt;

&lt;p&gt;My choices were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A database system / SQL&lt;/li&gt;
  &lt;li&gt;Creating and parsing plain text files&lt;/li&gt;
  &lt;li&gt;Using a data format like XML or JSON&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I went with JSON for a number of reasons:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;It’s very easy to work with&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It’s a standard, portable format that is widely used and well-understood&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There is good functionality for working with it in C++ (through a third-party
library)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One potential drawback could be that the file size may grow large with a lot of
audio files and associated analysis data. The program likely won’t be working
with many audio files at once however, and JSON is designed to store data in
much larger quantities than my program will be using.&lt;/p&gt;

&lt;p&gt;I used the &lt;a href=&quot;https://github.com/nlohmann/json&quot;&gt;following library&lt;/a&gt; to get the functionality required.&lt;/p&gt;

&lt;h3 id=&quot;changes-to-overall-code-design&quot;&gt;Changes to Overall Code Design&lt;/h3&gt;
&lt;p&gt;One change was to change my usages of unsigned integer types with the simple “int” signed integer type. &lt;a href=&quot;http://soundsoftware.ac.uk/c-pitfall-unsigned.html&quot;&gt;I learned about some bugs that unsigned integers can result in&lt;/a&gt;. Using ints for all integer types also simplifies the code and makes it more readable.&lt;/p&gt;

&lt;p&gt;I also made some other simple type-related revisions, such as using double for sample rate variables, both for the reason that it is consistent with the libraries I am using, and that it makes the values easier to work with when doing calculations.&lt;/p&gt;

&lt;p&gt;I also made more usage of the “auto” keyword in function bodies, which is considered good practice in modern C++. I plan to aim to write most of my code in this way, with the exception of some of audio code where I may have to focus more on efficiency and a more low-level way of doing things if necessary.&lt;/p&gt;

&lt;h3 id=&quot;moving-onwards&quot;&gt;Moving Onwards&lt;/h3&gt;
&lt;p&gt;My immediate plans moving forwards are as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Performing a sliding-window analysis of audio files, and storing the resulting data in the JSON data file&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Building Kd-tree from data and using KNN algorithm to find closest audio unit to current analysis frame. I plan to use the &lt;a href=&quot;https://github.com/jlblancoc/nanoflann&quot;&gt;following library&lt;/a&gt; for this&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Adding some more audio features, particularly spectral ones&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Liam Wyllie</name></author><summary type="html">Progress I have been implementing functionality for audio analysis and storage for the program. I am on schedule with this and will continue working on it next week as well.</summary></entry></feed>